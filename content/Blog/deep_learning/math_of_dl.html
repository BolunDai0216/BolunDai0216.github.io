<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Mathematics of Deep Learning Notes I</title>
    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW"
        crossorigin="anonymous"></script>

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Own Design -->
    <link rel="stylesheet" href="../../../css/base.css" />
    <link rel="stylesheet" href="../../../css/blog.css" />
    <script src="../../../js/utils.js"></script>

    <!-- Academicons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>

<body>
    <div id="app">
        <my-header></my-header>

        <div class="container-fluid">
            <div class="row">
                <div class="col-0 col-lg-2">
                    <p>

                    </p>
                </div>

                <div class="col-12 col-lg-8">
                    <h1 class="text-left blog-title" style="padding-left: 0%;">Mathematics of Deep Learning Notes I</h1>
                    <p class="blog-date" style="padding-left: 0%;">By Bolun Dai | Feb 27th, 2021</p>
                </div>
            </div>
        </div>

        <div class="container-fluid">
            <div class="row">
                <div class="col-12 col-lg-2">
                    <div id="side-list" class="list-group sticky-top">
                        <a class="list-group-item blog-side-item" href="#main">
                            Main Ingredients of Superivised Learning
                        </a>
                        <a class="list-group-item blog-side-item" href="#fourier-series">
                            ...
                        </a>
                        <a class="list-group-item blog-side-item" href="#fourier-transform">
                            ...
                        </a>
                        <a class="list-group-item blog-side-item" href="#discrete-fourier-transform">
                            ...
                        </a>
                        <a class="list-group-item blog-side-item" href="#applications">
                            ...
                        </a>
                    </div>
                </div>

                <div class="col-12 col-lg-8">
                    <div class="row" style="margin-top: 4%;">
                        <p class="text-left blog-content full-page-blog" style="padding-left: 0%; padding-right: 0%;">
                            The blog is derived from the notes I have from taking the
                            <a
                                href="https://www.notion.so/Mathematics-of-Deep-Learning-05cd9255f03842489083ec7cbb6338d5">
                                Mathematics of Deep Learning
                            </a> course by Joan Bruna at New York Univeristy.
                        </p>

                        <p class="h2 text-left full-page-blog" id="main"
                            style="color: white; font-family: Georgia; border-bottom: 2px solid white; padding-left: 0%; margin-right: 15%;">
                            Main Ingredients of Superivised Learning
                        </p>

                        <p class="text-left blog-content full-page-blog"
                            style="padding-left: 0%; padding-right: 0%; margin-top: 1%;">
                            A supervised learning problem usually consists of four components: the input domain
                            \(\chi\), the data distribution \(\nu\), the target function \(f^*\) and a loss (or risk)
                            \(\mathcal{L}\).
                        </p>

                        <p class="text-left blog-content full-page-blog"
                            style="padding-left: 0%; padding-right: 0%; margin-top: 1%;">
                            The input domain \(\chi\) is usually high dimension, \(\chi\in\mathbb{R}^d\). For the MNIST
                            dataset, inputs are of dimension \(28\times28 = 784\). The data distribution \(\nu\) is
                            defined over an input domain \(\chi\), \(\nu\in P(\chi)\), where \(P(\chi)\) is the set of
                            all probability distributions over the input domain \(\chi\). The target function \(f^*\)
                            maps inputs to scalar values, \(f^*: \chi\rightarrow\mathbb{R}\), i.e. for an image
                            \(\chi_i\), \(f^*\) maps it to \(1\) if it contains a cat, \(0\) otherwise. The loss (or
                            risk) \(\mathcal{L}\) is a functional
                            $$\mathcal{L}(f) = \mathbb{E}_{x\sim\nu}\Big[\ell\Big(f(x), f^*(x)\Big)\Big],$$
                            this is saying for data sampled from data distribution \(\nu\) the loss is a metric of the
                            difference between the learned function \(f\) and target function \(f^*\). For mean
                            squared error (MSE) loss, we have
                            $$\mathcal{L}(f) = \mathbb{E}_{x\sim\nu}\Big[\Big|f(x) - f^*(x)\Big|^2\Big] = \Big\|f(x) -
                            f^*(x)\Big\|_\nu^2,$$
                            here the loss \(\mathcal{L}\) is convex w.r.t the learned function \(f\).
                        </p>

                        <p class="text-left blog-content full-page-blog"
                            style="padding-left: 0%; padding-right: 0%; margin-top: 1%;">
                            The goal of supervised learning is to predict the target function \(f^*\) from finite number
                            of observations, under the assumption observations are sampled i.i.d from the data
                            distribution \(\nu\)
                            $$\nu = \Big\{\Big(x_i, f^*(x_i)\Big)\Big\}_i,\ x_i\underset{iid}{\sim}\nu.$$
                        </p>

                        <p class="text-left blog-content full-page-blog"
                            style="padding-left: 0%; padding-right: 0%; margin-top: 1%;">
                            Since we cannot know exactly how well an algorithm will work in practice
                            (the true "risk") because we don't know the true distribution of data that the algorithm
                            will work on, we can instead measure its performance on a known set of training data
                            (the "empirical" risk) (<a
                                href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">Wikipedia</a>). The
                            empirical risk is defined as
                            $$\widehat{\mathcal{L}}(f) = \frac{1}{L}\sum_{i=1}^{L}\ell\Big(f(x), f^*(x)\Big).$$
                        </p>

                        <p class="text-left blog-content full-page-blog"
                            style="padding-left: 0%; padding-right: 0%; margin-top: 1%;">
                            Instead of looking at any function \(f\) we are only finding for the target function within
                            a subset. We consider a hypothesis space of possible functions that approximates the target
                            function
                            $$\mathcal{F} \subseteq \Big\{f: \chi\rightarrow\mathbb{R}\Big\},$$
                            where it is a normed space. By saying it is a norm space we are saying a measurement of
                            length exists, in this case the "length" measurement denotes the complexity of the function.
                            Thus, \(\gamma: \mathcal{F}\rightarrow\mathbb{R}\), where \(\gamma(f)\) denotes how complex
                            the hypothesis \(f\in\mathcal{F}\) is. In the case of deep learning, \(\mathcal{F}\) can be
                            the set of neural networks of a certain architecture, i.e.
                            $$\mathcal{F} = \Big\{f: x\rightarrow\mathbb{R}\ \Big|\ f(x) = w_3g_2(w_2g_1(w_1x))\Big\},$$
                            where \(w_i\) is the weight vector of the \(i\)-th layer and \(g_i\) is the \(i\)-th
                            activation function. This denotes the set of neural networks with three layers, no bias and
                            two activation functions (no activation at the output layer). If we consider a ball
                            $$\mathcal{F}_\delta = \Big\{f\in\mathcal{F}\ \Big|\ \gamma(f)\leq\delta\Big\},$$
                            it is a convex set, we can then only look for functions within this ball.
                        </p>

                    </div>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                <hr style="height:4px; border-width:0; margin-top: 2%; color:white; background-color:white">
                <p class="text-left blog-content full-page-blog citation">
                    Credit to Joan Bruna, powered by <a href="https://www.mathjax.org">
                        <img title="Powered by MathJax" src="https://www.mathjax.org/badge/mj_logo.png"
                            style="border:0;" alt="Powered by MathJax" />
                    </a>
                </p>
            </div>
        </div>

        <my-footer></my-footer>

    </div>

    <script src="https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.js"></script>
    <script src="../../../js/page_components.js"></script>
    <script>
        var vm = new Vue({
            el: '#app',
            data: {

            }
        });
    </script>
</body>

</html>