I"<p>Gradient descent is a method for unconstrained, smooth convex optimization problems, such as</p>

\[\min_{x}\ \ f(x).\]

<p>Following the definition of a convex optimiztion problem: the objective (f) has to be a convex function over a convex domain. Along with the smooth assumption, we can say that (f(x)) is a convex and differentiable function over (\mathrm{dom}(f) = \mathbb{R}^n). Starting from an initial state (x^{(0)}\in\mathbb{R}^n), at each step the gradient descent algorithm performs an update</p>

\[x^{(k)} = x^{(k-1)} - t_k\nabla f(x^{(k-1)}),\ k=1, 2, 3, \cdots,\]

<p>here (t_k) is the (k)-th step size, (\nabla f(x^{(k-1)})) is the gradient of (f(x)) evaluated at (x^{(k-1)}) and (x^{(k)}) is the estimation of the solution after the (k)-th update. We perform the update iteratively until it converges to a solution (x^*), which approximately minimizes (f(x)).</p>

<p>For those coming from a machine learning background we know the term gradient descent from stochastic gradient descent (SGD), and the intuition behind SGD is simply going in the direction of the negative gradient. This is correct. However, this is a processed version, the original intuition behind gradient descent is different.</p>

<p>In the original flavor, we are approximating the function (f(x)) at (x^{(k)}) using a quadratic function, then we can find (\tilde{x}) such that it gives the minimal value to the quadratic function. Afterwards, we simply update the current estimation of the solution to be (\tilde{x}). Now the next problem becomes which quadratic approximation should we use? If we write the second-order Taylorâ€™s expansion of (f(x)) at (x^{(k-1)}) we have</p>

\[f(y)\approx f(x^{(k-1)}) + \nabla f^T(x^{(k-1)})(y - x^{(k-1)}) + \frac{1}{2}(y - x^{(k-1)})^T\nabla^2f^T(x^{(k-1)})(y - x^{(k-1)}).\]

<p>Then by estimating the Hessian (\nabla^2f^T(x^{(k-1)})) with (\frac{1}{t_k}I) we have</p>

\[f(y)\approx f(x^{(k-1)}) + \nabla f^T(x^{(k-1)})(y - x^{(k-1)}) + \frac{1}{2t_k}\|y - x^{(k-1)}\|_2^2.\]

<p>We can set (x^{(k)}) to be where the minimum value of the approximated quadratic function is. To do this, we take the derivative of the approximated quadratic function</p>

\[\frac{\partial f(y)}{\partial y} = \nabla{f(x^{(k)})} + \frac{1}{t_k}(y - x^{(k)}).\]

<p>By setting it to 0 we have (y^* = x^{(k)} = x^{(k-1)} - t_k\nabla f(x^{(k-1)})). This process is shown in the image below. Assume we are at the light blue point ((4, e^4)) on the blue curve (e^x). We do a quadratic approximation at ((4, e^4)) with (t=0.01), which gives us the red curve. We find the argmin of the red curve and find its projection on the blue curve, which gives us the red dot. Although the result is that same as going in the negative gradient direction, the thought process is not the same. The next problem that we face is how to define the step size (t_k). One thing to note is that with different step sizes the approximated quadratic function will also be different.</p>

<figure class="l-page">
    <img src="/assets/images/convex_opt/trapezoidal.png" alt="trapezoidal quadrature" />
</figure>
:ET