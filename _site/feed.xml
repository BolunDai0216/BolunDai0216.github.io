<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-04-21T08:33:27-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">BolunDai0216.github.io</title><entry><title type="html">An Introduction to Control Barrier Functions</title><link href="http://localhost:4000/2022/04/20/cbf.html" rel="alternate" type="text/html" title="An Introduction to Control Barrier Functions" /><published>2022-04-20T00:00:00-04:00</published><updated>2022-04-20T00:00:00-04:00</updated><id>http://localhost:4000/2022/04/20/cbf</id><content type="html" xml:base="http://localhost:4000/2022/04/20/cbf.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Introduction to Rigid Transformations</title><link href="http://localhost:4000/2022/04/20/rigid_transformation.html" rel="alternate" type="text/html" title="Introduction to Rigid Transformations" /><published>2022-04-20T00:00:00-04:00</published><updated>2022-04-20T00:00:00-04:00</updated><id>http://localhost:4000/2022/04/20/rigid_transformation</id><content type="html" xml:base="http://localhost:4000/2022/04/20/rigid_transformation.html">&lt;p&gt;In this blog, I want to talk give a review of rigid transformations, in specific rotation and translation. Although this is the most basic concept in robotics, I tend to confuse many concepts. Therefore, this to me is more like a cheatsheet. The content is developed from &lt;d-cite key=&quot;DBLP:books/daglib/0073732&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt;

&lt;h2 id=&quot;rigid-transformations&quot;&gt;Rigid Transformations&lt;/h2&gt;

&lt;p&gt;A rigid transformation in \(\mathbb{R}^3\) is a mapping \(g:\mathbb{R}^3\rightarrow\mathbb{R}^3\) that has the following properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Length is preserved \(\|g(p) - g(q)\| = \|p - q\|\) for all points \(p, q\in\mathbb{R}^3\).&lt;/li&gt;
  &lt;li&gt;The cross product is preserved \(g_*(v\times w) = g_*(v)\times g_*(w)\) for all vectors \(v, w\in\mathbb{R}^3\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to the above properties we can also prove that the inner product is preserved
\(v_1^Tv_2 = g_*(v_1)^Tg_*(v_2).\)&lt;/p&gt;

&lt;figure class=&quot;l-middle&quot;&gt;
    &lt;img src=&quot;/assets/images/random/rigid_transformation.gif&quot; alt=&quot;rigid transformation illustration&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;We can see from the above animation, that after translation and rotation the length of the two blue lines did not change (\(\|\cdot\|\) did not change), thus satisfying the first property. From the definition of inner product we have&lt;/p&gt;

\[\langle \mathbf{a}, \mathbf{b}\rangle = \|\mathbf{a}\|\|\mathbf{b}\|\cos(\theta)\]

&lt;p&gt;where \(\theta\) is the angle between \(\mathbf{a}\) and \(\mathbf{b}\). We can see that the angle between the two lines did not change (\(\cos(\theta)\) does not change), and since the length is also kept the same, the inner product is preserved. The rotation is in a 2D plane, but we can see that for the body coordinate system before and after the transformation (orange axes) the z-axis is the same, thus the cross product is preserved. This result generalizes to 3D rotations.&lt;/p&gt;

&lt;h2 id=&quot;rotations&quot;&gt;Rotations&lt;/h2&gt;

&lt;h3 id=&quot;properites&quot;&gt;Properites&lt;/h3&gt;

&lt;p&gt;The rotation matrix has the following properties
\(SO(3) = \{R\in\mathbb{R}^{3\times3}\ |\ RR^T = I, \mathrm{det}R = 1\},\)&lt;/p&gt;

&lt;p&gt;where \(SO(3)\) represents the special orthogonal group. Since the rotation matrix is a group under matrix multiplication it has some additional properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Closure: If \(R_1, R_2\) are rotation matrices then \(R_1R_2\) is also a rotation matrix.&lt;/li&gt;
  &lt;li&gt;Identity: The identity matrix is the identity element.&lt;/li&gt;
  &lt;li&gt;Inverse: Each rotation matrix has an inverse \(R^{-1} = R^T\).&lt;/li&gt;
  &lt;li&gt;Associativity: For rotation matrices \(R_1, R_2\) and \(R_3\) we have \((R_1R_2)R_3 = R_1(R_2R_3)\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also rotation matrix is a rigid transformation which means that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;R preserves length: \(\|Rq - Rp\| = \|p - q\|\) for all points \(p, q\in\mathbb{R}^3\).&lt;/li&gt;
  &lt;li&gt;R preserves orientation: \(R(v\times w) = (Rv)\times(Rw)\) for all vectors \(v, w\in\mathbb{R}^3\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representations&quot;&gt;Representations&lt;/h3&gt;

&lt;figure class=&quot;l-middle&quot;&gt;
    &lt;img src=&quot;/assets/images/random/rotation.gif&quot; alt=&quot;rotation illustration&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;We can see from the animation, that if we have a vector \(\mathbf{q}\) rotating with respect to another vector \(\omega\) with constant angular velocity \(\|\omega\|\), its linear velocity can be calculated as \(\dot{\mathbf{q}} = \omega\times\mathbf{q} = \hat{\omega}\mathbf{q}\). From the knowledge of linear systems (solution of \(\dot{x} = Ax\) is \(x(t) = e^{At}x_0\)) we have the trajectory of the vector as \(\mathbf{q}^\prime = \mathbf{q}(t) = e^{\hat{\omega}t}\mathbf{q}_0\). Additionally, if we have \(\|\omega\| = 1\), then we have \(t = \theta\) the rotation angle.&lt;/p&gt;

&lt;p&gt;From &lt;strong&gt;Euler’s theorem&lt;/strong&gt; we know that for any rotation matrix \(R\in SO(3)\), it can be seen to be equivalent to rotation about a fixed axis \(\omega\in\mathbb{R}^3\) through an angle \(\theta\in[0, 2\pi]\). To represent the rotation matrix this way we can use the exponential map&lt;/p&gt;

\[R = e^{\hat{w}\theta} = I + \frac{\hat{\omega}}{\|\omega\|}\sin{(\|\omega\|\theta)} + \frac{\hat{\omega}^2}{\|\omega\|}\bigg(1 - \cos{((\|\omega\|\theta))}\bigg)\]

&lt;p&gt;where \(\hat{\omega}\) is defined as&lt;/p&gt;

\[\hat{\omega} = \begin{bmatrix}
0 &amp;amp; -\omega_3 &amp;amp; \omega_2\\
\omega_3 &amp;amp; 0 &amp;amp; -\omega_1\\
-\omega_2 &amp;amp; \omega_1 &amp;amp; 0
\end{bmatrix}\in so(3) = \{S\in\mathbb{R}^{3\times3}\ |\ S^T = -S\}.\]

&lt;p&gt;This mapping is many-to-one, which says that there are many \(e^{\hat{w}\theta}\) corresponding to one \(R\), and for every \(R\in SO(3)\) we have more than one \(e^{\hat{w}\theta}\). To obtain \(\omega\) and \(\theta\) from \(R\) we have&lt;/p&gt;

\[\theta = \cos^{-1}\Big(\frac{\mathrm{trace}(R) - 1}{2}\Big)\ \mathrm{and}\ \omega =
\frac{1}{2\sin\theta}\begin{bmatrix}
r_{32} - r_{23}\\
r_{13} - r_{31}\\
r_{21} - r_{12}
\end{bmatrix},\]

&lt;p&gt;we can see that the pair \((\theta, \omega)\) and \((2\pi - \theta, -\omega)\) can both obtain \(R\), thus makes this mapping many-to-one. Note that in the case where \(\theta = 0\) we can pick any arbitrary \(\omega\), which all makes \(R = I\).&lt;/p&gt;

&lt;h3 id=&quot;better-understanding-what-rotation-matrices-represent&quot;&gt;Better understanding what rotation matrices represent&lt;/h3&gt;

&lt;figure class=&quot;l-middle&quot;&gt;
    &lt;img src=&quot;/assets/images/random/twoViewofR.png&quot; alt=&quot;rotation illustration&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Rotation matrices can be seen as a mapping from describing a point in one coordinate frame to describing it in another coordinate frame. For the equation \(x_g^\prime = Rx_b\), we can see \(x_b\) as a point specified in a body frame \(b\) and \(x_g^\prime\) being the same point, however, specified in the coordinates of the global fixed frame \(g\). The rotation matrix \(R\) then serves as a mapping between these two representations.&lt;/p&gt;

&lt;p&gt;Another way to see rotation matrices is to see it as an action, to rotate points in the same frame from one configuration to another. Using the equation \(x_g^\prime = Rx_g\), we can see \(x_g\) as the the point before applying the rotation \(R\) and \(x_g^\prime\) the point after applying rotation \(R\).&lt;/p&gt;

&lt;p&gt;With these two views, we can now better understand how to represent rotations w.r.t the global fixed frame and the body frame. Now we ask, if sequentially applying two rotations \(R_1\) and \(R_2\) both specified in the body frame, where would the point end up in the global frame? Denote \(x_2\) as the final position in the global frame and \(x_0\) the initial position in the global frame, we want to find the rotation matrix between \(x_2\) and \(x_0\). First, we denote the initial position in the global frame as \(x_0^g\), where the superscript \(g\) denotes that this is represented in the global frame. After applying \(R_1\) we have&lt;/p&gt;

\[x_1^g = R_1x_0^g.\]

&lt;p&gt;One thing to note is that in the body frame the coordinates of the point is always the same, which is equal to \(x_0^g\). Thus we have&lt;/p&gt;

\[R_1^Tx_1^g = x_1^{1} = x_0^g,\]

&lt;p&gt;where the superscript 1 denotes the point represented in the body frame after rotation \(R_1\). If we see \(R_1\) as a mapping between points in the frame 1 and the global frame \(g\), we have the following relationship&lt;/p&gt;

\[R_1^Tx^g = x^{1}\ \mathrm{and}\ x^g = R_1x^{1}.\]

&lt;p&gt;Then we can see if we apply \(R_2\) which is w.r.t the frame \(b1\) we can have&lt;/p&gt;

\[x_2^{1} = R_2x_1^{1}\ \rightarrow\ R_2^Tx_2^{1} = x_2^{2} = x_1^{1} = x_0^g,\]

&lt;p&gt;similar to before this utilizes the fact that the coordinates of the point is kept constant in the current body frame. Finally, if we use \(R_1\) to map \(x_2^{1}\) to \(x_2^g\) we can have&lt;/p&gt;

\[R_1x_2^{1} = x_2^g,\]

&lt;p&gt;then we can have&lt;/p&gt;

\[x_2^{g} = R_1R_2x_0{g}.\]

&lt;p&gt;An interesting question would be after we get \(x_2^{g}\) if we rotate about the global frame using \(R_3\) and then rotate about the body frame using \(R_4\) what point will we end up at. If call the point after applying \(R_3\): \(x_3\), we can easily have&lt;/p&gt;

\[x_3^g = R_3x_2^g = R_3R_1R_2x_0^{g} = R_3R_1R_2x_3^{3},\]

&lt;p&gt;which again utilizes the fact that the body frame coordinates do not change \(x_3^{3} = x_2^{2} = x_1^{1} = x_0^g\). Also we obtain a mapping from the global frame and the body frame \(3\): \(x^g = R_3R_1R_2x^3\). Then if we apply \(R_4\), and get to point 4 we can have&lt;/p&gt;

\[x_4^{3} = R_4x_3^{3}\ \rightarrow\ R_4^Tx_4^{3} = x_3^{3} = x_4^{4} = x_0^g.\]

&lt;p&gt;To map \(x_4^{3}\) into the global frame we can have&lt;/p&gt;

\[R_3R_1R_2x_4^{3} = R_3R_1R_2R_4x_4^{4} = x_4^g\ \rightarrow\ R_4^TR_2^TR_1^TR_3^Tx_4^g = x_0^g\ \rightarrow\ x_4^g = R_3R_1R_2R_4x_0^g.\]

&lt;p&gt;Following this line of thought, I conjecture that we can completely decouple the rotations about the global frame and the body frame. I am pretty sure this is proved and shown somewhere, if anyone reading this has any information please let me know.&lt;/p&gt;

&lt;h2 id=&quot;rigid-motions-in-3d&quot;&gt;Rigid Motions in 3D&lt;/h2&gt;

&lt;p&gt;In this section I will talk about rigid motions which includes both translation and rotation. I will present three ways of describing such a motion, namely, homogeneous transformation, twist and screw motions, and we will see how these three descriptions are related.&lt;/p&gt;

&lt;h3 id=&quot;homogeneous-representations&quot;&gt;Homogeneous Representations&lt;/h3&gt;

&lt;p&gt;Any rigid motion can be represented by a translation \(p\) and a rotation \(R\). If we perform such a transformation \(g(\cdot)\), we can relate the position before and after the transformation as&lt;/p&gt;

\[q^\prime = g(q) = p + Rq.\]

&lt;p&gt;And for a vector \(v\) we have \(g_*(v) = Rv\). This forms the special Euclidean group&lt;/p&gt;

\[SE(3) = \{(p, R)\ |\ p\in\mathbb{R}^3, R\in SO(3)\} = \mathbb{R}^3\times SO(3).\]

&lt;p&gt;As before we can see this from two ways. If \(g(\cdot)\) is seen as an action then both \(q^\prime\) and \(q\) are measured in the same coordinate frame. If we see this as a mapping between frames, then \(q^\prime\) and \(q\) are measured in different frames. Later I will consolidate this using an example. Before that, I want to introduce the homogeneous representation of points and vectors. For a point \(p\) and vector \(v\) we can have their homogeneous representation as
\(\bar{p} = \begin{bmatrix}
p_1\\
p_2\\
p_3\\
1
\end{bmatrix},\ \bar{v} = \begin{bmatrix}
v_1\\
v_2\\
v_3\\
0
\end{bmatrix},\)&lt;/p&gt;

&lt;p&gt;here I would like to draw your attention to the last element, for points the last element is \(1\) and for vectors the last element is \(0\). The we can homogeneous representation of \(SE(3)\) as&lt;/p&gt;

\[\bar{g} = \begin{bmatrix}
R &amp;amp; p\\
\mathbf{0} &amp;amp; 1
\end{bmatrix}.\]

&lt;p&gt;It is proved that \(SE(3)\) is a group, therefore, it satisfies the following properties&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If \(g_1, g_2\in SE(3)\), then \(g_1g_2\in SE(3)\).&lt;/li&gt;
  &lt;li&gt;It has an identity element which is \(I_{4\times4}\).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There exists an inverse for \(g = (p, R)\in SE(3)\), defined as
\(g^{-1} = (-R^Tp, R^T)\ \rightarrow\ \bar{g}^{-1} = \begin{bmatrix}
  R^T &amp;amp; -R^Tp\\
  \mathbf{0} &amp;amp; 1
  \end{bmatrix}.\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The composition rule
\(\bar{g}_{ac} = \bar{g}_{ab}\bar{g}_{bc} = \begin{bmatrix}
  R_{ab}R_{bc} &amp;amp; R_{ab}p_{bc} + p_{ab}\\
  \mathbf{0} &amp;amp; 1
  \end{bmatrix}\)
is associative.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also we can prove that is a rigid transformation, which infers&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(g\) preserves distance between points \(\|g(p) - g(q)\| = \|p - q\|\), for all \(p, q\in\mathbb{R}^3\).&lt;/li&gt;
  &lt;li&gt;\(g\) preserves orientation between vectors \(g_*(v\times w) = g_*(v)\times g_*(w)\), for all \(v, w\in\mathbb{R}^3\).&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;l-middle&quot;&gt;
    &lt;img src=&quot;/assets/images/random/twoViewofg.png&quot; alt=&quot;rotation illustration&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Now I want to use an example to better understand the two views of this homogeneous representation. Consider rotating \(\theta\) about the vertical line going though point \((l_x, l_y, 0)\), how can we find the transformation \(g_{ab}\) such that \(x_g\) is the position of the point before and \(x_g^\prime\) is the position after, which are all represented in the global fixed frame. We can see this as two steps, a rotation \(R\) and a translation \(p\), the rotation is w.r.t the vertical line \((0, 0, 0)\) and the translation is from \((0, 0, 0)\) to \((l_x, l_y, 0)\). Denoting a point \(x\) with the coordinates in the body frame (light purple frame above) as \(x_b\), we define&lt;/p&gt;

\[x_b^1 = Rx_b.\]

&lt;p&gt;And after the translation we have&lt;/p&gt;

\[x_g^\prime = x_b^1 + p = Rx_b + p.\]

&lt;p&gt;Therefore we have&lt;/p&gt;

\[x_g^\prime = p + Rx_b\ \rightarrow\ \bar{g} = \begin{bmatrix}
R &amp;amp; p\\
0 &amp;amp; 1
\end{bmatrix} = \begin{bmatrix}
\cos\theta &amp;amp; -\sin\theta &amp;amp; 0 &amp;amp; l_x\\
\sin\theta &amp;amp; \cos\theta &amp;amp; 0 &amp;amp; l_y\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}.\]

&lt;p&gt;Also if we start at point \(x_g = x_b\), where \(x_g\) is measured in the global frame, this gives you a way to calculate the position after applying \(R\) and \(p\)&lt;/p&gt;

\[x_g^\prime = p + Rx_g.\]

&lt;h3 id=&quot;twists&quot;&gt;Twists&lt;/h3&gt;

&lt;p&gt;In the previous sections, we talked about how to rotate rigid bodies, however, all of them were rotating about an axis that passes through the global fixed frame, in this part in addition to introducing a more compact way of representing rigid body motion, I will also talk about how to rotate about axis that do not pass through the origin of the fixed frame. In the homogeneous coordinates we can write the pure rotation with unit velocity about axis \(\omega\in\mathbb{R}^3\)&lt;/p&gt;

&lt;p&gt;\(\dot{p}(t) = \omega\times(p(t) - q)\)
as
\(\begin{bmatrix}
\dot{p}\\
1
\end{bmatrix} = \begin{bmatrix}
\hat{\omega} &amp;amp; -\omega\times q\\
0 &amp;amp; 1
\end{bmatrix}\begin{bmatrix}
p\\
1
\end{bmatrix} = \hat{\xi}\begin{bmatrix}
p\\
1
\end{bmatrix}\ \longleftrightarrow \dot{\bar{p}} = \hat{\xi}\bar{p}.\)&lt;/p&gt;

&lt;p&gt;Thus the solution \(\bar{p}(t) = \exp{(\hat{\xi}t)}\bar{p}(0)\) enables a mapping from the initial location to the location after \(t\) radians of rotation \(\exp{(\hat{\xi}t)}\). Note that here \(q\) is a point on the axis \(\omega\) given in the global coordinates. For pure translations we have&lt;/p&gt;

\[\hat{\xi} = \begin{bmatrix}
0 &amp;amp; v\\
0 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;where \(v\) is a vector representing the translation velocity. We define twists as the group&lt;/p&gt;

\[\hat{\xi} \in se(3) := \{(v, \hat{\omega})\ |\ v\in\mathbb{R}^3, \hat{\omega}\in so(3)\} =
\begin{bmatrix}
\hat{\omega} &amp;amp; v\\
0 &amp;amp; 1
\end{bmatrix}\in\mathbb{R}^{4\times4},\]

&lt;p&gt;with the twist coordinates \(\xi := (v,\ \omega)\). We can prove that for every member of \(se(3)\) we can find its corresponding element in \(SE(3)\) using an exponential mapping&lt;/p&gt;

\[\begin{align*}
e^{\hat{\xi}\theta} &amp;amp;= \begin{bmatrix}
I &amp;amp; v\theta\\
0 &amp;amp; 1
\end{bmatrix} &amp;amp; \omega = 0\\
e^{\hat{\xi}\theta} &amp;amp;= \begin{bmatrix}
e^{\hat{\omega}\theta} &amp;amp; (I - e^{\hat{\omega}\theta})(\omega\times v) +
\omega\omega^Tv\theta\\
0 &amp;amp; 1
\end{bmatrix} &amp;amp; \omega \neq 0.
\end{align*}\]

&lt;p&gt;Also for every \(p, R\) we can find its corresponding twist \(\hat{\xi}\in se(3)\) and \(\theta\in\mathbb{R}\), for \(R = I\) we have&lt;/p&gt;

\[\begin{align*}
\hat{\xi} &amp;amp;= \begin{bmatrix}
0 &amp;amp; p/\|p\|\\
0 &amp;amp; 0
\end{bmatrix} &amp;amp;\theta = \|p\|.
\end{align*}\]

&lt;p&gt;If \(R \neq I\), then first we can solve for \(\hat{w}\) and \(\theta\) by solving \(e^{\hat{\omega}\theta} = R\), which was mentioned above. Then we can solve for \(v\)&lt;/p&gt;

\[v = \Big[(I - e^{\hat{\omega}\theta})\hat{\omega} + \omega\omega^T\theta\Big]^{-1}p.\]

&lt;p&gt;Note, here if we write \(p(\theta) = \exp{(\hat{\xi}\theta)}p(0)\), then \(p(\theta)\) and \(p(0)\) are specified in the same reference frame. If we want to specify \(p(0)\) in the body frame then we have&lt;/p&gt;

\[p(\theta) = \exp{(\hat{\xi}\theta)}g_{gb}(0)p^b(0)\]

&lt;p&gt;where \(g_{gb}\) gives the mapping between coordinates in the body and the global frame at \(t = 0\).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Twist&lt;/th&gt;
      &lt;th&gt;Linear System&lt;/th&gt;
      &lt;th&gt;Comments&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\dot{p}(t) = \omega\times(p(t) - q)\)&lt;/td&gt;
      &lt;td&gt;\(\dot{x}(t) = \omega(x(t) - O)\)&lt;/td&gt;
      &lt;td&gt;\(\xi = \begin{bmatrix}-\omega\times q\\ \omega\end{bmatrix} = \begin{bmatrix}v\\ \omega\end{bmatrix}\), differential equations governing the movement&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\dot{\bar{p}} = \hat{\xi}\bar{p}\)&lt;/td&gt;
      &lt;td&gt;\(\dot{x} = Ax\)&lt;/td&gt;
      &lt;td&gt;\(\hat{\xi} = \begin{bmatrix}\hat{\omega} &amp;amp; -\omega\times q\\ 0 &amp;amp; 1\end{bmatrix}\), differential equation in vector form&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\bar{p}(t) = \exp{(\hat{\xi}t)}\bar{p}(0)\)&lt;/td&gt;
      &lt;td&gt;\(x(t) = \exp{(At)}x(0)\)&lt;/td&gt;
      &lt;td&gt;\(\exp{(\hat{\xi}t)} = \begin{bmatrix}e^{\hat{\omega}\theta} &amp;amp; (I - e^{\hat{\omega}\theta})(\omega\times v) + \omega\omega^Tv\theta\\ 0 &amp;amp; 1\end{bmatrix}\), analytic solution to differential equation.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;screw-motion&quot;&gt;Screw Motion&lt;/h3&gt;

&lt;p&gt;Here we state &lt;strong&gt;Chasles’ Theorem&lt;/strong&gt;: every rigid body motion can be realized by a rotation about an axis \(\omega\) combined with a translation parallel to that axis. We assume that the rotation is of \(\theta\) radians and the translation is of amount \(d\), also we can find a point on \(\omega\) and call it \(q\).&lt;/p&gt;

&lt;p&gt;We can see that this movement resembles the movement of a screw. We can describe the screw movement using: &lt;em&gt;pitch&lt;/em&gt; as \(h:=d/\theta\), the &lt;em&gt;axis&lt;/em&gt; of the screw movement as&lt;/p&gt;

\[l = \{q + \lambda\omega\ |\ \lambda\in\mathbb{R}\}\]

&lt;p&gt;and a &lt;em&gt;magnitude&lt;/em&gt; \(M = \theta\) when there is rotation and \(M = \infty\) when there is only translation.&lt;/p&gt;

&lt;p&gt;For a given screw motion we can find its rigid transfomation as&lt;/p&gt;

\[\begin{align*}
g &amp;amp;= \begin{bmatrix}
e^{\hat{\omega}\theta} &amp;amp; (I - e^{\hat{\omega}\theta})q + h\theta\omega\\
0 &amp;amp; 1
\end{bmatrix} &amp;amp; \mathrm{with\ rotation}\\
g &amp;amp;= \begin{bmatrix}
I &amp;amp; \theta v\\
0 &amp;amp; 1
\end{bmatrix} &amp;amp; \mathrm{pure\ translation}
\end{align*}\]

&lt;p&gt;where \(v\) is the vector of translation. Additionally, when given a twist we can also find its corresponding screw, and given a screw we can find its corresponding twist.&lt;/p&gt;

&lt;h2 id=&quot;velocity-of-a-rigid-body&quot;&gt;Velocity of a Rigid Body&lt;/h2&gt;

&lt;h3 id=&quot;rotational-velocity&quot;&gt;Rotational Velocity&lt;/h3&gt;

&lt;p&gt;First we define the spatial coordinate frame \(A\) which is fixed and the body coordinate frame \(B\) which is moving. If we define a point \(q_b\) in the body frame, and a rotation motion \(R_{ab}(t)\), then we can have the trajectory of the point in spatial coordinates as&lt;/p&gt;

\[q_a(t) = R_{ab}(t)q_b.\]

&lt;p&gt;By deriving that both \(R^{-1}\dot{R}\) and \(\dot{R}R^{-1}\) are skew symmetric matrices, we can get the angular velocity in both the spatial coordinate \(\hat{\omega}^s\) and the instantaneous body coordinate \(\hat{\omega}^b\) (since the body coordinate is changing due to rotation)&lt;/p&gt;

\[v_{q_a}(t) = \dot{R}_{ab}(t)q_b =
\underbrace{\dot{R}_{ab}(t)R_{ab}^{-1}(t)}_{:=\ \hat{\omega}_{ab}^s}R_{ab}(t)q_b =
\hat{\omega}_{ab}^sR_{ab}(t)q_b = \hat{\omega}_{ab}q_a.\]

&lt;p&gt;The instantaneous body angluar velocity is defined as seeing the spatial angular velocity vector in the body frame,&lt;/p&gt;

\[v_{q_b}(t) := R_{ab}^Tv_{q_a}(t)\ \ \ \ \hat{\omega}_{ab}^b = R_{ab}^{-1}\dot{R}_{ab}\]

&lt;p&gt;which is not the angluar velocity of the rigid body w.r.t the body frame, the latter is always zero.&lt;/p&gt;

&lt;h3 id=&quot;rigid-body-velocity&quot;&gt;Rigid Body Velocity&lt;/h3&gt;

&lt;p&gt;For a rigid motion plan \(g_{ab}(t)\in SE(3)\) we can get the spatial velocity as a twist \(\hat{V}_{ab}^s\in se(3)\)&lt;/p&gt;

\[\hat{V}_{ab}^s = \dot{g}_{ab}g_{ab}^{-1}\ \ \ \ V_{ab}^s = \begin{bmatrix}
v_{ab}^s\\
\omega_{ab}^s
\end{bmatrix} = \begin{bmatrix}
-\dot{R}_{ab}R_{ab}^Tp_{ab} + \dot{p}_{ab}\\
(\dot{R}_{ab}R_{ab}^T)^\vee
\end{bmatrix}\]

&lt;p&gt;Thus the velocity of a point \(q_a\) can be found as&lt;/p&gt;

\[v_{q_a} = \hat{V}_{ab}^sq_a = \omega_{ab}^s\times q_a + v_{ab}^s.\]

&lt;p&gt;Here we can interpret \(\omega_{ab}^s\) as the instantaneous angular velocity of the body as viewed in the spatial frame and \(v_{ab}^s\) as the velocity of a (possibly imaginary) point on the rigid body which is traveling through the origin of the spatial frame at time \(t\). The body velocity is defined as&lt;/p&gt;

\[\hat{V}_{ab}^b = g_{ab}^{-1}\dot{g}_{ab}\ \ \ \ V_{ab}^b = \begin{bmatrix}
v_{ab}^b\\
\omega_{ab}^b
\end{bmatrix} = \begin{bmatrix}
R_{ab}^T\dot{p}_{ab}\\
(R_{ab}^T\dot{R}_{ab})^\vee
\end{bmatrix}\]

&lt;p&gt;which for \(v_{ab}^b\) can be interpretted as the velocity of the origin of the body coordinate frame relative to the spatial frame, as viewed in the current body frame; for \(\omega_{ab}^b\) we can see it as the angular velocity of the coordinate frame, also as viewed in the current body frame. We can also define an adjoint transformation to relate the spatial and body velcoties&lt;/p&gt;

\[V_{ab}^s = \begin{bmatrix}
v_{ab}^s\\
\omega_{ab}^s
\end{bmatrix} = \begin{bmatrix}
R_{ab} &amp;amp; \hat{p}_{ab}R_{ab}\\
0 &amp;amp; R_{ab}
\omega_{ab}^b
\end{bmatrix}\begin{bmatrix}
v_{ab}^b\\
\omega_{ab}^b
\end{bmatrix} = Ad_{g}V_{ab}^b.\]

&lt;h3 id=&quot;coordinate-transformation&quot;&gt;Coordinate Transformation&lt;/h3&gt;

&lt;p&gt;For three coordinate frames A, B and C, we can have the relationship between their spatial velocity as&lt;/p&gt;

\[V_{ac}^s = V_{ab}^s + Ad_{g_{ab}}V_{bc}^s\]

&lt;p&gt;similarly we can get the relationship between their body velocities as&lt;/p&gt;

\[V_{ac}^b = Ad_{g_{bc}^{-1}}V_{ab}^b + V_{bc}^b.\]

&lt;p&gt;This relationship can also be used to represent twists before and after applying a rigid motion&lt;/p&gt;

\[\xi^\prime = Ad_g\xi\ \ \mathrm{or}\ \ \hat{\xi}^\prime = g\hat{\xi}g^{-1}.\]

&lt;p&gt;Some additional properties include&lt;/p&gt;

\[\begin{align*}
V_{ab}^{b} &amp;amp;= V_{ba}^{s}\\
V_{ab}^{b} &amp;amp;= -Ad_{g_{ba}}V_{ba}^{b}
\end{align*}\]

&lt;h2 id=&quot;wrenches&quot;&gt;Wrenches&lt;/h2&gt;

&lt;p&gt;A wrench is defined as a force/moment pair \(F\in\mathbb{R}^6\)&lt;/p&gt;

\[F = \begin{bmatrix}
f\\
\tau
\end{bmatrix}.\]

&lt;p&gt;Given its dual nature with twist we have the relationship mapping wrenches between different coordinates systems as&lt;/p&gt;

\[F_c = Ad_{g_{bc}}^TF_b.\]

&lt;p&gt;And for the same wrench the instantaneous work is the same in both the spatial and body frame&lt;/p&gt;

\[\delta W = V^b\cdot F^b = V^s\cdot F^s,\]

&lt;p&gt;note that here \(\cdot\) represents the dot product.&lt;/p&gt;

&lt;p&gt;Using &lt;strong&gt;Poinsot&lt;/strong&gt;’s Theorem we can see every collection of wrenches applied to a rigid body is equivalent to a force applied along a fixed axis plus a torque about the same axis. Thus there exists a direct mapping between wrenches and screw motions.&lt;/p&gt;

&lt;d-byline&gt;&lt;/d-byline&gt;

&lt;p class=&quot;citation&quot;&gt;
    Powered by &lt;a href=&quot;https://www.mathjax.org&quot;&gt;
    &lt;img title=&quot;Powered by MathJax&quot; src=&quot;https://www.mathjax.org/badge/mj_logo.png&quot; style=&quot;border:0;&quot; alt=&quot;Powered by MathJax&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html">In this blog, I want to talk give a review of rigid transformations, in specific rotation and translation. Although this is the most basic concept in robotics, I tend to confuse many concepts. Therefore, this to me is more like a cheatsheet. The content is developed from .</summary></entry><entry><title type="html">Walking Without Thinking About It Notes</title><link href="http://localhost:4000/2022/04/20/walking.html" rel="alternate" type="text/html" title="Walking Without Thinking About It Notes" /><published>2022-04-20T00:00:00-04:00</published><updated>2022-04-20T00:00:00-04:00</updated><id>http://localhost:4000/2022/04/20/walking</id><content type="html" xml:base="http://localhost:4000/2022/04/20/walking.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">PyMuJoCoBase</title><link href="http://localhost:4000/2022/04/20/pymujocobase.html" rel="alternate" type="text/html" title="PyMuJoCoBase" /><published>2022-04-20T00:00:00-04:00</published><updated>2022-04-20T00:00:00-04:00</updated><id>http://localhost:4000/2022/04/20/pymujocobase</id><content type="html" xml:base="http://localhost:4000/2022/04/20/pymujocobase.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Modular &amp;amp; Minimal PyTorch Implementation of Proximal Policy Optimization</title><link href="http://localhost:4000/2022/04/20/ppo_torch.html" rel="alternate" type="text/html" title="Modular &amp;amp; Minimal PyTorch Implementation of Proximal Policy Optimization" /><published>2022-04-20T00:00:00-04:00</published><updated>2022-04-20T00:00:00-04:00</updated><id>http://localhost:4000/2022/04/20/ppo_torch</id><content type="html" xml:base="http://localhost:4000/2022/04/20/ppo_torch.html"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Cart-pole Swing-up Task Using MPC</title><link href="http://localhost:4000/2022/04/20/cp_swingup_mpc.html" rel="alternate" type="text/html" title="Cart-pole Swing-up Task Using MPC" /><published>2022-04-20T00:00:00-04:00</published><updated>2022-04-20T00:00:00-04:00</updated><id>http://localhost:4000/2022/04/20/cp_swingup_mpc</id><content type="html" xml:base="http://localhost:4000/2022/04/20/cp_swingup_mpc.html">&lt;p&gt;In this blog, I want to swing-up a cart-pole system using a model predictive control (MPC) based method. When I was initially shown this problem, I thought this was a toy problem and I will be able to solve in a few minutes. Then I realized that in school, all I have been taught was about linear MPC, and for cartpole related problems all I know is how to balance it on the top, assuming that it starting close to the top, so that the linearization is still valid. As for the case of swinging-up the cartpole, this stems firmly in the realm of nonlinear control. Which I have also learned. One of the more classical ways to deal with this would then be to linearize along the trajectory, such as using iLQR. However, when given state and control constraints, e.g. limit control effort and not letting the cartpole sway to far, iLQR losses its appeal. There are ways to incorporate constraints into iLQR, such as&lt;d-cite key=&quot;DBLP:conf/iros/HowellJM19&quot;&gt;&lt;/d-cite&gt;, which seems like an overkill for such a simple problem.&lt;/p&gt;

&lt;p&gt;I then did some reading and I found a way to do constrained nonlinear control: using direct collocation&lt;d-cite key=&quot;DBLP:journals/siamrev/Kelly17&quot;&gt;&lt;/d-cite&gt;. By tweaking the formulation a bit I was able to find a way to perform a trapezoidal-collocation-based-MPC to swing-up a cartpole system and that is what I will be presenting in this blog.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Now let’s first define the system and the problem. The dynamics of the cart-pole system can be defined as&lt;/p&gt;

\[\dot{\mathbf{x}} = \begin{bmatrix}
\dot{x}\\
\dot{\theta}\\
\ddot{x}\\
\ddot{\theta}
\end{bmatrix} = \begin{bmatrix}
\dot{x}\\
\dot{\theta}\\
\frac{m_p\sin{\theta}(l\dot{\theta}^2 + g\cos{\theta})}{m_c + m_p\sin^2{\theta}}\\
\frac{-m_pl\dot{\theta}^2\cos{\theta}\sin{\theta} - (m_c + m_p)g\sin{\theta}}{l(m_c +
m_p\sin^2{\theta})}
\end{bmatrix} + \begin{bmatrix}
0\\
0\\
\frac{1}{m_c + m_p\sin^2{\theta}}\\
\frac{-1}{l(m_c + m_p\sin^2{\theta})}
\end{bmatrix}u,\]

&lt;p&gt;where \(m_c\) is the weight of the cart, \(m_p\) is the weight of the pole, \(\ell\) is the length of the pole, \(x\) is the center of mass position of the cart, \(\theta\) is the orientation of the pole and \(u\) is the control action which in this case is a force acting on the cart. The control problem is to swing-up the pole till it balances at the upright position. This can be captured using the following cost function&lt;/p&gt;

\[J = \int_{0}^{T}{(\mathbf{x}(t) - x^*)^TQ(\mathbf{x}(t) - x^*) + u(t)^TRu(t)dt},\]

&lt;p&gt;where \(Q\) and \(R\) are weight matrices, \(T\) is the preview horizon and \(x^*\) is the target state, which in this case is \([1,\ \pi,\ 0,\ 0]^T\). To make the problem more realistic, the cart position and control efforts are all constrained, i.e. \(x\in[-a,\ a]\) and \(u\in[-b,\ b]\). This problem is a nonlinear constrained optimization problem which is quite hard to solve, if written as-is I don’t even know how to give it to a solver (if anyone reading this has a solution please let me know). In the next section, we will look into how using a trapezoidal collocation method we can transform the problem into something more tractable.&lt;/p&gt;

&lt;h2 id=&quot;trapezoidal-collocation&quot;&gt;Trapezoidal Collocation&lt;/h2&gt;

&lt;p&gt;The key idea here is to approximate integrals using trapezoidal quadrature. There are other ways to approximate it, as mentioned in&lt;d-cite key=&quot;DBLP:journals/siamrev/Kelly17&quot;&gt;&amp;lt;/dt-cite&amp;gt;, however, given its simplicity we will focus on the trapezoidal approximation. Like any other idea, to understand the concept all you need is the right image.&lt;/d-cite&gt;&lt;/p&gt;

&lt;figure class=&quot;l-page&quot;&gt;
    &lt;img src=&quot;/assets/images/control_theory/trapezoidal.png&quot; alt=&quot;trapezoidal quadrature&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Here we can use the system dynamics \(\dot{x} = f(x)\) as an example. If we want to integrate the system dynamics from \(t_k\) to \(t_{k+1}\),&lt;/p&gt;

\[\int_{t_k}^{t_{k+1}}{\dot{x}} = \int_{t_k}^{t_{k+1}}{f(x)},\]

&lt;p&gt;is equivalent to calculating the area under the curve \(f(x)\) from \(t_k\) to \(t_{k+1}\). Which we can easily see can be approximated by a trapezoidal quadrature denoted by the yellow region. Using the formula of trapezoidal area we can get an approximation of the system dynamics&lt;/p&gt;

\[\int_{t_k}^{t_{k+1}}{\dot{x}} = \frac{t_{k+1} - t_k}{2}\Big(f(x(t_k)) + f(x(t_{k+1}))\Big) \approx x(t_{k+1}) - x(t_k),\]

&lt;p&gt;where the last approximation is obtained by the fundamental theorem of calculus. By discretizing the system dynamics and cost function using a trapezoidal approximation we can easily write out the aforementioned nonlinear constrained optimization problem, which we will discuss in the next section.&lt;/p&gt;

&lt;h2 id=&quot;reformulation&quot;&gt;Reformulation&lt;/h2&gt;

&lt;p&gt;Using the aforementioned trapezoidal approximation, we can transform the cost from an integration over the entire trajectory to a summation at a few states, which are called knot points. The step-wise cost is&lt;/p&gt;

\[w(t) = (\mathbf{x}(t) - x^*)^TQ(\mathbf{x}(t) - x^*) + u(t)^TRu(t).\]

&lt;p&gt;If we take the integral between \(t_k\) and \(t_{k+1}\) we have&lt;/p&gt;

\[\int_{t_k}^{t_{k+1}}{w(t)dt} \approx \frac{t_{k+1} - t_k}{2}\Big(w(t_k) + w(t_{k+1})\Big).\]

&lt;p&gt;If we pick a series of timesteps \(h_k = t_{k+1} - t_k\), for the entire preview horizon we can have&lt;/p&gt;

\[\int_{0}^{T}{w(t)dt} = \sum_{i=0}^{N-1}{\frac{h_k}{2}\Big(w(t_k) + w(t_{k+1})\Big)}.\]

&lt;p&gt;Similarly, to ensure the state dynamics are respected, an equality constraint needs to be added. Following the same formulation as the last section we have a set of equality constraints, one at each knot point,&lt;/p&gt;

\[\frac{h_k}{2}\Big(f(x(t_k)) + f(x(t_{k+1}))\Big) = x(t_{k+1}) - x(t_k), \forall i = 1, \cdots, N-1.\]

&lt;p&gt;Now we can write out the reformulated problem&lt;/p&gt;

\[\begin{align*}
\min_{x(\cdot), u(\cdot)}\ &amp;amp;\ \sum_{i=0}^{N-1}{\frac{h_k}{2}\Big(w(t_k) + w(t_{k+1})\Big)}\\
s.t.\ &amp;amp;\ \frac{h_k}{2}\Big(f(x(t_k)) + f(x(t_{k+1}))\Big) =
x(t_{k+1}) - x(t_k)\\
&amp;amp;\ -\mathbf{a} \leq x(t_k) \leq \mathbf{a}\\
&amp;amp;\ -b \leq u(t_k) \leq b.\\
&amp;amp;\ x_0 = 0, \forall i = 1, \cdots, N-1.
\end{align*}\]

&lt;p&gt;For the original direct collocation approach, the cost is only the control squared and there is an additional terminal state constraint. However, in a MPC framework, this approach is too “fragmented”. Intuitively, one can think that under such a setup, in order the minimize the cost, it would be best to plan to only reach the top at the end of the preview horizon. Since in MPC the preview horizon is moving, this will make the system never attempt to actually reach the target, but always “preparing” to.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;figure class=&quot;l-page&quot;&gt;
    &lt;img src=&quot;/assets/images/control_theory/cartpole_solution.png&quot; alt=&quot;cartpole solution&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Using the aforementioned approach we can see that the cart-pole system is able to balance at the top. The control is saturated at 10N. The target is \([1.0,\ \pi,\ 0.0,\ 0.0]\). Though not shown here, the simulation is ran for 10 seconds and the pole is firmly balanced at the top. We see that this approach truly is able to solve the cartpole swing-up task. One thing to note is that when running SQP there is no need to run till a solution is found, in this case only four iteration is allowed at each timestep.&lt;/p&gt;

&lt;p&gt;A few notes on the implementation. I implemented this in Matlab, due to not being able to get scipy’s optimization package to work, while fmincon works right out the box, credit to Mathworks. If anyone is able to get this working on scipy please let me know, I am dying to know what I did wrong. To ensure the system dynamics are as close to reality as possible, it was integrated using ode45. This shows that even if the dynamics are simplified when using trapezoidal collocation, the resulting control is still adequate in dealing with real dynamics.&lt;/p&gt;

&lt;d-byline&gt;&lt;/d-byline&gt;

&lt;p class=&quot;citation&quot;&gt;
    Powered by &lt;a href=&quot;https://www.mathjax.org&quot;&gt;
    &lt;img title=&quot;Powered by MathJax&quot; src=&quot;https://www.mathjax.org/badge/mj_logo.png&quot; style=&quot;border:0;&quot; alt=&quot;Powered by MathJax&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html">In this blog, I want to swing-up a cart-pole system using a model predictive control (MPC) based method. When I was initially shown this problem, I thought this was a toy problem and I will be able to solve in a few minutes. Then I realized that in school, all I have been taught was about linear MPC, and for cartpole related problems all I know is how to balance it on the top, assuming that it starting close to the top, so that the linearization is still valid. As for the case of swinging-up the cartpole, this stems firmly in the realm of nonlinear control. Which I have also learned. One of the more classical ways to deal with this would then be to linearize along the trajectory, such as using iLQR. However, when given state and control constraints, e.g. limit control effort and not letting the cartpole sway to far, iLQR losses its appeal. There are ways to incorporate constraints into iLQR, such as, which seems like an overkill for such a simple problem.</summary></entry><entry><title type="html">Linear Time Invariant Systems</title><link href="http://localhost:4000/2022/01/01/lti.html" rel="alternate" type="text/html" title="Linear Time Invariant Systems" /><published>2022-01-01T00:00:00-05:00</published><updated>2022-01-01T00:00:00-05:00</updated><id>http://localhost:4000/2022/01/01/lti</id><content type="html" xml:base="http://localhost:4000/2022/01/01/lti.html">&lt;p&gt;To understand control theory we need a medium. We can see how different motor torques effect the walking motion of a biped robot but the correlation would be convoluted for a beginner. The default choice of such a medium for control theory noobs is a linear time invariant (LTI) system. The next question would be what makes LTI systems special? Let’s start with some of its properties. A LTI system has three main properties: homogeneity, additivity (or commonly known as superposition) and as the name suggests time invariance. One may ask I see the time invariance, where is the linear? The “linear” part is constructed by the homogeneity and additivity. In the following sections we  will look into these three properties.&lt;/p&gt;

&lt;p&gt;First, let’s take a look at homogeneity. What this means is that for a LTI system, if the output of the system is \(y(t)\) when given input \(x(t)\), then the output will be \(ay(t)\) when the input is \(ax(t)\). For example, if I weight \(70kg\) the scale will show that I weight \(70kg\), if I weight \(2\times70 = 140kg\) the scale will show that I weight \(2\times70 = 140kg\).&lt;/p&gt;

&lt;p&gt;Then, let’s talk about additivity. What this means if that for a LTI system, if the system outputs \(y_1(t)\) when given input \(x_1(t)\) and outputs \(y_2(t)\) when given input \(x_2(t)\), when given input \(x_1(t) + x_2(t)\) it will output \(y_1(t) + y_2(t)\). Also using the scale example, if two people of weight \(70kg\) each stand on the scale at the same time the scale will show \(70 + 70 = 140kg\).&lt;/p&gt;

&lt;p&gt;Finally, for time invariance it means that if the output of the system is \(y(t)\) when given input \(x(t)\), the output will be \(y(t-a)\) when given input \(x(t-a)\). Using the scale example, if I maintained the same weight of \(70kg\) throughout the day, the scale will show \(70kg\) no matter if its in the morning or \(11\)pm.&lt;/p&gt;

&lt;p&gt;For a system that is subject to an unit impulse \(\delta(t)\) we can have its output as \(y(t)\). If the system is subject to three impulses \(\delta(t)\), \(2\delta(t-1)\) and \(3\delta(t-2)\), using the properties of a LTI system we can say the output at time \(t\) will be&lt;/p&gt;

\[\mathrm{output}(t) = y(t) + 2y(t-1) + 3y(t-2).\]

&lt;p&gt;For a continuous input signal \(x(t)\) we can see it as applying the impulse \(x(a)\delta(t-a)\) at time \(t = a\). Then also using the properties of a LTI system we have&lt;/p&gt;

\[\mathrm{output}(t) = \int_{-\infty}^{\infty}x(a)y(t-a)da\]

&lt;p&gt;with \(x(a) = 0\) for \(a \leq 0\) and \(y(t-a) = 0\) for \(a \geq 0\). This is for accounting for the control signal to be \(0\) for all negative time, and the output to be \(0\) for all negative time. This operation is called a convolution and usually denotes as \(x*y\).&lt;/p&gt;

&lt;d-byline&gt;&lt;/d-byline&gt;

&lt;p class=&quot;citation&quot;&gt;
    Powered by &lt;a href=&quot;https://www.mathjax.org&quot;&gt;
    &lt;img title=&quot;Powered by MathJax&quot; src=&quot;https://www.mathjax.org/badge/mj_logo.png&quot; style=&quot;border:0;&quot; alt=&quot;Powered by MathJax&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html">To understand control theory we need a medium. We can see how different motor torques effect the walking motion of a biped robot but the correlation would be convoluted for a beginner. The default choice of such a medium for control theory noobs is a linear time invariant (LTI) system. The next question would be what makes LTI systems special? Let’s start with some of its properties. A LTI system has three main properties: homogeneity, additivity (or commonly known as superposition) and as the name suggests time invariance. One may ask I see the time invariance, where is the linear? The “linear” part is constructed by the homogeneity and additivity. In the following sections we will look into these three properties.</summary></entry><entry><title type="html">Mathematics of Deep Learning Notes</title><link href="http://localhost:4000/2021/02/27/math_of_dl.html" rel="alternate" type="text/html" title="Mathematics of Deep Learning Notes" /><published>2021-02-27T00:00:00-05:00</published><updated>2021-02-27T00:00:00-05:00</updated><id>http://localhost:4000/2021/02/27/math_of_dl</id><content type="html" xml:base="http://localhost:4000/2021/02/27/math_of_dl.html">&lt;p&gt;The blog is derived from the notes I have from taking the &lt;a href=&quot;https://www.notion.so/Mathematics-of-Deep-Learning-05cd9255f03842489083ec7cbb6338d5&quot;&gt;Mathematics of Deep Learning&lt;/a&gt; course by Joan Bruna at New York Univeristy.&lt;/p&gt;

&lt;h2 id=&quot;main-ingredients-of-superivised-learning&quot;&gt;Main Ingredients of Superivised Learning&lt;/h2&gt;

&lt;p&gt;A supervised learning problem usually consists of four components: the input domain \(\chi\), the data distribution \(\nu\), the target function \(f^*\) and a loss (or risk) \(\mathcal{L}\).&lt;/p&gt;

&lt;p&gt;The input domain \(\chi\) is usually high dimension, \(\chi\in\mathbb{R}^d\). For the MNIST dataset, inputs are of dimension \(28\times28 = 784\). The data distribution \(\nu\) is defined over an input domain \(\chi\), \(\nu\in P(\chi)\), where \(P(\chi)\) is the set of all probability distributions over the input domain \(\chi\). The target function \(f^*\) maps inputs to scalar values, \(f^*: \chi\rightarrow\mathbb{R}\), i.e. for an image \(\chi_i\), \(f^*\) maps it to \(1\) if it contains a cat, \(0\) otherwise. The loss (or risk) \(\mathcal{L}\) is a functional&lt;/p&gt;

\[\mathcal{L}(f) = \mathbb{E}_{x\sim\nu}\Big[\ell\Big(f(x), f^*(x)\Big)\Big],\]

&lt;p&gt;this is saying for data sampled from data distribution \(\nu\) the loss is a metric of the difference between the learned function \(f\) and target function \(f^*\). For mean squared error (MSE) loss, we have&lt;/p&gt;

\[\mathcal{L}(f) = \mathbb{E}_{x\sim\nu}\Big[\Big|f(x) - f^*(x)\Big|^2\Big] = \Big\|f(x) - f^*(x)\Big\|_\nu^2,\]

&lt;p&gt;here the loss \(\mathcal{L}\) is convex w.r.t the learned function \(f\).&lt;/p&gt;

&lt;p&gt;The goal of supervised learning is to predict the target function \(f^*\) from finite number of observations, under the assumption observations are sampled i.i.d from the data distribution \(\nu\)&lt;/p&gt;

\[\nu = \Big\{\Big(x_i, f^*(x_i)\Big)\Big\}_i,\ x_i\underset{iid}{\sim}\nu.\]

&lt;p&gt;Since we cannot know exactly how well an algorithm will work in practice (the true “risk”) because we don’t know the true distribution of data that the algorithm will work on, we can instead measure its performance on a known set of training data (the “empirical” risk) (&lt;a href=&quot;https://en.wikipedia.org/wiki/Empirical_risk_minimization&amp;quot;&amp;gt;Wikipedia&quot;&gt;Wikipedia&lt;/a&gt;). The
empirical risk is defined as&lt;/p&gt;

\[\widehat{\mathcal{L}}(f) = \frac{1}{L}\sum_{i=1}^{L}\ell\Big(f(x), f^*(x)\Big).\]

&lt;p&gt;Instead of looking at any function \(f\) we are only searching for the target functions within a subset. We consider a hypothesis space of possible functions that approximates the target function&lt;/p&gt;

\[\mathcal{F} \subseteq \Big\{f: \chi\rightarrow\mathbb{R}\Big\},\]

&lt;p&gt;where it is a normed space. By saying it is a norm space we are saying a measurement of length exists, in this case the “length” measurement denotes the complexity of the function. Thus, \(\gamma: \mathcal{F}\rightarrow\mathbb{R}\), where \(\gamma(f)\) denotes how complex the hypothesis \(f\in\mathcal{F}\) is. In the case of deep learning, \(\mathcal{F}\) can be the set of neural networks of a certain architecture, i.e.&lt;/p&gt;

\[\mathcal{F} = \Big\{f: x\rightarrow\mathbb{R}\ \Big|\ f(x) = w_3g_2(w_2g_1(w_1x))\Big\},\]

&lt;p&gt;where \(w_i\) is the weight vector of the \(i\)-th layer and \(g_i\) is the \(i\)-th activation function. This denotes the set of neural networks with three layers, no bias and two activation functions (no activation at the output layer). If we consider a ball&lt;/p&gt;

\[\mathcal{F}_\delta = \Big\{f\in\mathcal{F}\ \Big|\ \gamma(f)\leq\delta\Big\},\]

&lt;p&gt;it is a convex set, we can then only look for functions within this ball.&lt;/p&gt;

&lt;p&gt;Now the principle of stuctural risk minimization can be introduced. When fitting a function to a dataset overfitting can occur, this usually happens when the learned function has high complexity. When overfitting occurs, the learned model will have a really good performance on the training set and will generally perform badly on the test set. Therefore, to avoid overfitting, we would like the learned model to have low complexity while minimizing the training error. These two objectives can be written as&lt;/p&gt;

\[\min_{\gamma(f)\leq\delta}\widehat{\mathcal{L}}(f),\]

&lt;p&gt;this is called the constrained form, where the emprical risk is minimized while restricting the complexity of the learned model. We can also write it in the penalized form&lt;/p&gt;

\[\min_{f\in\mathcal{F}}\widehat{\mathcal{L}}(f) + \lambda\gamma(f),\]

&lt;p&gt;to penalize models with large complexity. Or the interpolating form&lt;/p&gt;

\[\begin{align*}
\min &amp;amp;\ \gamma(f)\\
\mathrm{s.t.} &amp;amp;\ \widehat{\mathcal{L}}(f) = 0.
\end{align*}\]

&lt;p&gt;Note the interpolating form makes sense only when the data has no false labels.&lt;/p&gt;

&lt;h2 id=&quot;basic-decomposition-of-errors&quot;&gt;Basic Decomposition of Errors&lt;/h2&gt;

&lt;p&gt;Suppose using the constrained form we found a function \(\tilde{f}\in\mathcal{F}_\delta\), such that&lt;/p&gt;

\[\widehat{\mathcal{L}}(\tilde{f}) \leq \min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) + \epsilon_o.\]

&lt;p&gt;This is saying the empirical loss of the candidate function \(\tilde{f}\) is only \(\epsilon_0\) away from the best solution in the ball (the problem is almost solved). Now let’s take a look at how good is our candidate function \(\tilde{f}\) w.r.t. to the original loss (not the empirical loss)&lt;/p&gt;

\[\begin{align*}
&amp;amp;\ \mathcal{L}(\tilde{f}) - \min_{f\in\mathcal{F}}\mathcal{L}(f)\\
=&amp;amp;\ \mathcal{L}(\tilde{f}) - \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) +
\min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) - \min_{f\in\mathcal{F}}\mathcal{L}(f)\\
=&amp;amp;\ \mathcal{L}(\tilde{f}) - \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) + \epsilon_a &amp;amp;
\epsilon_a = \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) -
\min_{f\in\mathcal{F}}\mathcal{L}(f)\\
=&amp;amp;\ \widehat{\mathcal{L}}(\tilde{f}) - \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) +
\mathcal{L}(\tilde{f}) - \widehat{\mathcal{L}}(\tilde{f}) + \epsilon_a\\
\leq&amp;amp;\ \min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) -
\min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) + \mathcal{L}(\tilde{f}) -
\widehat{\mathcal{L}}(\tilde{f}) + \epsilon_a + \epsilon_o &amp;amp;
\text{used inequality above}\\
\leq&amp;amp;\ 2\sup_{f\in\mathcal{F}_\delta}\Big|\mathcal{L}(f) - \widehat{\mathcal{L}}(f)\Big| +
\epsilon_a + \epsilon_o &amp;amp; \text{see proof below}\\
=&amp;amp;\ \epsilon_s + \epsilon_a + \epsilon_o &amp;amp; \epsilon_s =
2\sup_{f\in\mathcal{F}_\delta}\Big|\mathcal{L}(f) - \widehat{\mathcal{L}}(f)\Big|
\end{align*}\]

&lt;p&gt;We now see why&lt;/p&gt;

\[\min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) - \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) \leq \sup_{f\in\mathcal{F}_\delta}\Big|\mathcal{L}(f) - \widehat{\mathcal{L}}(f)\Big|.\]

&lt;p&gt;Assume we have&lt;/p&gt;

\[\DeclareMathOperator*{\argmin}{arg\,min} \argmin_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) = f_1\ \ \ \text{and}\ \ \ \argmin_{f\in\mathcal{F}_\delta}\mathcal{L}(f) = f_2.\]

&lt;p&gt;For the case&lt;/p&gt;

\[\min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) &amp;gt; \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f),\]

&lt;p&gt;we have&lt;/p&gt;

\[\begin{align*}
\min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) -
\min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) &amp;amp;= \widehat{\mathcal{L}}(f_1) -
\mathcal{L}(f_2)\\
&amp;amp;\leq \widehat{\mathcal{L}}(f_2) -
\mathcal{L}(f_2)\\
&amp;amp;\leq \sup_{f\in\mathcal{F}_\delta}\Big|\mathcal{L}(f) -
\widehat{\mathcal{L}}(f)\Big|,
\end{align*}\]

&lt;p&gt;which is denoted by the right figure.&lt;/p&gt;

&lt;figure class=&quot;l-page&quot;&gt;
    &lt;img src=&quot;/assets/images/deep_learning/errorProof.png&quot; alt=&quot;Illustration of error proof&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;For the case&lt;/p&gt;

\[\min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) \leq \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f),\]

&lt;p&gt;we have&lt;/p&gt;

\[\min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) - \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) \leq 0 \leq \sup_{f\in\mathcal{F}_\delta}\Big|\mathcal{L}(f) - \widehat{\mathcal{L}}(f)\Big|,\]

&lt;p&gt;which is denoted by the left figure. Thus we have&lt;/p&gt;

\[\min_{f\in\mathcal{F}_\delta}\widehat{\mathcal{L}}(f) - \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) \leq \sup_{f\in\mathcal{F}_\delta}\Big|\mathcal{L}(f) - \widehat{\mathcal{L}}(f)\Big|.\]

&lt;p&gt;Now let’s take a look at each of the three errors: approximation error \(\epsilon_a\), statisical error \(\epsilon_s\) and optimization error \(\epsilon_o\). We have the approximation error as&lt;/p&gt;

\[\begin{align*}
\epsilon_a &amp;amp;= \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f) -
\min_{f\in\mathcal{F}}\mathcal{L}(f)\\
&amp;amp;= \min_{f\in\mathcal{F}_\delta}\|f - f^*\|_2^2 -
\min_{f\in\mathcal{F}}\|f - f^*\|_2^2.
\end{align*}\]

&lt;p&gt;The first term is the approximation error of the set of functions \(f\in\mathcal{F}_\delta\). The second term is the approximation error using any function \(f\). Using the universal approximation theorem we know for neural networks of more than two layers the second term is equal to \(0\). Thus we have&lt;/p&gt;

\[\epsilon_a = \min_{f\in\mathcal{F}_\delta}\mathcal{L}(f),\]

&lt;p&gt;which decreases as \(\delta\) increases. The optimization error \(\epsilon_o\) is present due to the use of algorithms such as SGD and ADAM when finding the weights of a neural network. The statisical error \(\epsilon_s\) has the form of&lt;/p&gt;

\[\epsilon_s = 2\sup_{f\in\mathcal{F}_\delta}\Big|\mathcal{L}(f) - \widehat{\mathcal{L}}(f)\Big| \sim O\Big(\sqrt{\frac{\delta}{L}}\Big),\]

&lt;p&gt;where \(\delta\) represents the complexity and \(L\) represents the number of samples. To understand this we can write out the two terms \(\mathcal{L}(f)\) and \(\widehat{\mathcal{L}}(f)\) as&lt;/p&gt;

\[\mathcal{L}(f) = \mathbb{E}_{x\sim\nu}\Big[f(x)\Big]\ \ \ \text{and}\ \ \ \widehat{\mathcal{L}}(f) = \widehat{\mathbb{E}}_{x\sim\nu}\Big[f(x)\Big].\]

&lt;p&gt;From the law of big numbers we know that when the number of samples \(L\) is large we have \(\mathcal{L}(f) \approx \widehat{\mathcal{L}}(f)\), thus small statisitical error \(\epsilon_s\). Note \(\epsilon_s\) is not about a single function \(f\in\mathcal{F}_\delta\) but all of the function in \(\mathcal{F}_\delta\). Therefore, if \(\delta\) increases the number of functions that belongs to the set \(\mathcal{F}_\delta\) also increases. Thus making the statisical error \(\epsilon_s\) larger.&lt;/p&gt;

&lt;h2 id=&quot;curse-of-dimensionality&quot;&gt;Curse of Dimensionality&lt;/h2&gt;

&lt;p&gt;This section will discuss how approximation error \(\epsilon_a\), statisical error \(\epsilon_s\) and optimization error \(\epsilon_o\) will behave as the input dimension changes.&lt;/p&gt;

&lt;p&gt;First, let’s talk about the statisitical curse. Assume we observe \(\{x_i, f^*(x_i)\}_i\), for \(i = 1, \cdots, L\) and \(x_R\in\mathbb{R}^d\). The question we would like to ask is for \(f^*\) belonging to a certain hypothesis class and the dimension of the input \(d\), what is the number of observations \(L\) required to estimate \(f^*\) up to a certain accuracy.&lt;/p&gt;

&lt;p&gt;Suppose the target function \(f^*\) is a linear function: \(f^*(x) = x^T\theta\), \(\theta\in\mathbb{R}^d\), then instead of looking into all classes of functions, the hypothesis space would only include linear functions&lt;/p&gt;

\[\mathcal{F} = \Big\{f: \mathbb{R}^d\rightarrow\mathbb{R}\ \Big|\ f(x) = x^T\theta\Big\}.\]

&lt;p&gt;Now the function is parametrized by a \(d\)-dimension vector, since it is a linear function we can treat the observations as \(L\) linear equations with \(d\) unknowns&lt;/p&gt;

\[\Big\{x_i^T\theta_\mathrm{est} = f^*(x_i)\Big\}_{i = 1 \sim L}.\]

&lt;p&gt;From our knowledge of linear algebra we know that as long as \(L\geq d\) we can find a definitive solution.&lt;/p&gt;

&lt;p&gt;What about functions that are only locally linear? A function that is locally linear can be
defined as&lt;/p&gt;

\[\|f(x) - f(\tilde{x})\| \leq \beta\|x - \tilde{x}\|,\]

&lt;p&gt;which is the same as being \(\beta\)-Lipschitz. Thus, the function space we search for is&lt;/p&gt;

\[\mathcal{F} = \Big\{f: \mathbb{R}^d\rightarrow\mathbb{R}\ \Big|\ f\ \text{is Lipschitz}\Big\}.\]

&lt;p&gt;For this class a functions a good candidate for determining the complexity is something like the variance in curvature. We don’t want the function to be very curly. To describe this mathematically we can use the Lipschitz constant&lt;/p&gt;

\[\gamma(f) = \mathrm{Lip}(f) + \|f\|_\infty,\]

&lt;p&gt;I am not sure why the second term is there, but that does not interfere with the intuition provided. Just to restate the goal \(\forall\epsilon\), we would like to find \(f\in\mathcal{F}\), such that \(\|f - f^*\|\leq\epsilon\), from \(L\) i.i.d samples \(\{x_i, f^*(x_i)\}_{i=1\sim L}\). And the question would be how large of a \(L\) would be required to achieve error \(\epsilon\). The answer is \(L\sim\epsilon^{-d}\). To find such a function we would basically need to grid the domain, therefore for a \(1\)-dimension hypercube we would need \(1/\epsilon = \epsilon^{-1}\) samples to achieve accuracy \(\epsilon\), for a \(2\)-dimension problem we would need \(\epsilon^{-2}\) samples and for a \(d\)-dimension problem it would require \(\epsilon^{-d}\) samples. Now let’s prove that this is in fact the upper bound of the number of samples required to achieve error \(\epsilon\). Given \(\{x_i, f^*(x_i)\}_{i=1\sim L}\), consider&lt;/p&gt;

\[\DeclareMathOperator*{\argmin}{arg\,min} \hat{f} = \argmin_{f\in\mathcal{F}}\Big\{\mathrm{Lip}(f)\ \Big|\ f(x_i) = f^*(x_i), \forall i\Big\},\]

&lt;p&gt;and assume \(\mathrm{Lip}(f^*) = 1\). Then we can say \(\mathrm{Lip}(\hat{f}) \leq 1\), because we already know that a function with Lipschitz coefficient \(1\) goes through all of the points and \(\hat{f}\) is the function with the smallest Lipschitz coefficient that goes through all of the points. Thus, we have \(\mathrm{Lip}(\hat{f}) \leq \mathrm{Lip}(f^*) = 1\). For a random data point \(x\) and \(\bar{x}\) being the training data closest to \(x\) we have&lt;/p&gt;

\[\begin{align*}
|\hat{f}(x) - f^*(x)| &amp;amp;= |\hat{f}(x) - f^*(x) - \hat{f}(\bar{x}) + \hat{f}(\bar{x}) -
f^*(\bar{x}) + f^*(\bar{x})|\\
&amp;amp;\leq |\hat{f}(x) - \hat{f}(\bar{x})| + |\hat{f}(\bar{x}) - f^*(\bar{x})| + |f^*(\bar{x}) -
f^*(x)|\\
&amp;amp;= |\hat{f}(x) - \hat{f}(\bar{x})| + |f^*(\bar{x}) - f^*(x)| &amp;amp; \hat{f}(\bar{x}) -
f^*(\bar{x}) = 0\\
&amp;amp;\leq 2\|x - \bar{x}\| &amp;amp; \mathrm{Lip}(\hat{f}) \leq \mathrm{Lip}(f^*) = 1
\end{align*}\]

&lt;p&gt;If we take the expectation of the square on both sides we have&lt;/p&gt;

\[\mathbb{E}_{x\sim\nu}\Big[|\hat{f}(x) - f^*(x)|^2\Big] \leq \mathbb{E}_{x\sim\nu}\Big[4\|x - \bar{x}\|^2\Big] = w_2^2(\nu, \hat{\nu}_L) \sim L^{-1/d} = \epsilon,\]

&lt;p&gt;this is saying the square expectation has the same representation as the wassertein distance between the real data distribution \(\nu\) and the empirical data distribution for \(L\) samples \(\hat{\nu}_L\). Such a wassertein distance is on the order of \(L^{-1/d}\). Then we have&lt;/p&gt;

\[L^{-1/d} = \epsilon\ \rightarrow\ L = \epsilon^{-d}.\]

&lt;p&gt;Also using maximum descrepancy (did not go into detail) one can establish the lower bound is also \(\epsilon^{-d}\), this means unless we grid the domain little can be done.&lt;/p&gt;

&lt;p&gt;To summarize, for linear functions we would only need a small amount of samples but this set of functions can be too restricitive, while looking a Lipschitz functions we would need exponentionally more samples, which is simply too much. Therefore, the majority of the functions discussed in this set of notes will be between these two categories.&lt;/p&gt;

&lt;p&gt;Now let’s talk about the curse of dimensionality in approximation. To state the results, for smooth functions we can get a nice approximation with small neural networks. Where for functions that are not very smooth we would need an infinite number of neurons.&lt;/p&gt;

&lt;p&gt;For the curse of dimensionality in optimization we would also need to grid the domain, which requires an exponentional number of operations. One set of functions that breaks this curse is convex functions.&lt;/p&gt;

&lt;d-byline&gt;&lt;/d-byline&gt;

&lt;p class=&quot;citation&quot;&gt;
    Credit to Joan Bruna, powered by &lt;a href=&quot;https://www.mathjax.org&quot;&gt;
    &lt;img title=&quot;Powered by MathJax&quot; src=&quot;https://www.mathjax.org/badge/mj_logo.png&quot; style=&quot;border:0;&quot; alt=&quot;Powered by MathJax&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html">The blog is derived from the notes I have from taking the Mathematics of Deep Learning course by Joan Bruna at New York Univeristy.</summary></entry><entry><title type="html">Fourier Analysis</title><link href="http://localhost:4000/2021/02/19/fourier-analysis.html" rel="alternate" type="text/html" title="Fourier Analysis" /><published>2021-02-19T00:00:00-05:00</published><updated>2021-02-19T00:00:00-05:00</updated><id>http://localhost:4000/2021/02/19/fourier-analysis</id><content type="html" xml:base="http://localhost:4000/2021/02/19/fourier-analysis.html">&lt;p&gt;The goal of this blog post is to discuss breifly what is Fourier transform and Fourier series, show the intuition behind them and talk about how to obtain them in real life.&lt;/p&gt;

&lt;h1 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h1&gt;

&lt;p&gt;What Fourier series and Fourier transform both enable us to do is to represent functions as a sum of cosine and sine functions. We will see that we can treat cosine and sine functions of different frequencies as basis: the Fourier basis, and by projecting the original function onto this Fourier basis we can figure out how large a portion does a specific sine or cosine wave occupy in the original signal. We will see how we can do this on periodic (Fourier series) and non-periodic signals (Fourier transform).&lt;/p&gt;

&lt;p&gt;Before going into the details, first let’s review inner products and its role in projections, then we will show that using a specific definition of inner products sine and cosine waves form an orthogonal basis. In the Euclidean space, the inner product can be seen as a measurement of how similar two vectors are. The inner product of two vectors in a Euclidean space can be calulated as&lt;/p&gt;

\[\langle\vec{a},\vec{b}\rangle = \|\vec{a}\|\|\vec{b}\|\cos\theta,\]

&lt;p&gt;where \(\theta\) is the angle between \(\vec{a}\) and \(\vec{b}\). We can see if the two vectors are pointing in the same direction \(\cos\theta = 1\), if they are pointing in the opposite direction then \(\cos\theta = -1\). Thus, the more similar the two vectors, the larger the inner product, vice versa. If \(\vec{b}\) is a unit vector then \(\|\vec{a}\|\|\vec{b}\|\cos\theta\) is the magnitude of the projection vector of \(\vec{a}\) onto \(\vec{b}\). If we have two coordinate systems \(x, y\) and \(u, v\), for a vector in the \(x, y\) coordinate system it can be represented as&lt;/p&gt;

\[\vec{p} = (a, b) := a\frac{\vec{x}}{\|\vec{x}\|^2} + b\frac{\vec{y}}{\|\vec{y}\|^2}.\]

&lt;p&gt;To represent it in the \(u, v\) coordinate system, we would have&lt;/p&gt;

\[\vec{p} = (\langle\vec{p},\vec{u}\rangle, \langle\vec{p},\vec{v}\rangle) := \langle\vec{p},\vec{u}\rangle\frac{\vec{u}}{\|\vec{u}\|^2} + \langle\vec{p},\vec{v}\rangle\frac{\vec{v}}{\|\vec{v}\|^2}.\]

&lt;p&gt;The reason the we have the norm square in the denominator is because the projection vector of \(\vec{a}\) onto \(\vec{b}\) is actually \(\|\vec{a}\|\cos\theta\vec{e}\), where \(\vec{e}\) is the unit vector along the direction of \(\vec{b}\). Thus we have&lt;/p&gt;

\[\langle\vec{a},\vec{b}\rangle\frac{\vec{b}}{\|\vec{b}\|^2} = \|\vec{a}\|\|\vec{b}\|\cos\theta\frac{\vec{b}}{\|\vec{b}\|^2} = \|\vec{a}\|\cos\theta\frac{\vec{b}}{\|\vec{b}\|} = \|\vec{a}\|\cos\theta\vec{e}\]

&lt;p&gt;The above definition of the inner product is defined in the Euclidean space. Since we are using sine and cosine functions as basis, we are no longer in a Euclidean space, now we are in the realm of a Hilbert space (this is not important, you can just see this as a way of saying the coordinate system is made up of functions). In the Hilbert space, the inner product can be defined as&lt;/p&gt;

\[\Big\langle f(x), g(x)\Big\rangle = \int_b^a f(x)\overline{g}(x)dx, x\in\mathbb{C}\]

&lt;p&gt;where \(\overline{g}(x)\) denotes the conjugate of \(g(x)\). Recall the conjugate of \(a + ib\) is simply \(a - ib\). If we are using a computer to get data from \(f(x)\) and \(g(x)\), we are not able to get the data at each \(x\). What we can do is for every fixed iterval \(\Delta{x}\) we sample a data point and we can calculate the inner product as&lt;/p&gt;

\[\Big\langle f(x), g(x)\Big\rangle \approx \sum_{i=1}^{n}{f_i\overline{g}_i\Delta{x}}.\]

&lt;h1 id=&quot;fourier-series&quot;&gt;Fourier Series&lt;/h1&gt;

&lt;p&gt;First we show the result, for \(f(x)\) defined on \([-\pi, \pi]\) we have:&lt;/p&gt;

\[f(x) = \frac{A_0}{2} + \sum_{k=1}^{\infty}\Big[A_k\cos(kx) + B_k\sin(kx)\Big],\]

&lt;p&gt;where we have&lt;/p&gt;

\[A_k = \frac{1}{\pi}\int_{-\pi}^{\pi}{f(x)\cos(kx)dx} = \frac{1}{\|\cos(kx)\|^2}\Big\langle f(x), \cos(kx)\Big\rangle\]

&lt;p&gt;and&lt;/p&gt;

\[B_k = \frac{1}{\pi}\int_{-\pi}^{\pi}{f(x)\sin(kx)dx} = \frac{1}{\|\sin(kx)\|^2}\Big\langle f(x), \sin(kx)\Big\rangle.\]

&lt;p&gt;We can see that by simply projecting \(f(x)\) onto the sine and cosine functions we can get the corresponding Fourier coefficients. If \(f(x)\) is defined on \([0, L]\) then we would need to use a different set of sine and cosine functions. This set of functions need to have periods of \(L\) or \(L/k\). Note that the period of \(\sin(kx)\) is \(2\pi/k\). Thus, if \(\sin(mx)\) has period \(L/k\) we have&lt;/p&gt;

\[\frac{2\pi}{m} = \frac{L}{k} \rightarrow m = \frac{2k\pi}{L}.\]

&lt;p&gt;Thus, we can write the Fourier series as&lt;/p&gt;

\[f(x) = \frac{A_0}{2} + \sum_{k=1}^{\infty}\Big[A_k\cos(\frac{2k\pi}{L}x) + B_k\sin(\frac{2k\pi}{L}x)\Big],\]

&lt;p&gt;with&lt;/p&gt;

\[A_k = \frac{2}{L}\int_{0}^{L}{f(x)\cos(\frac{2k\pi}{L}x)dx} = \frac{1}{\|\cos(\frac{2k\pi}{L}x)\|^2}\Big\langle f(x), \cos(\frac{2k\pi}{L}x)\Big\rangle\]

&lt;p&gt;and&lt;/p&gt;

\[B_k = \frac{2}{L}\int_{0}^{L}{f(x)\sin(\frac{2k\pi}{L}x)dx} = \frac{1}{\|\sin(\frac{2k\pi}{L}x)\|^2}\Big\langle f(x), \sin(\frac{2k\pi}{L}x)\Big\rangle.\]

&lt;p&gt;Note that even though \(f(x)\) is defined on \([0, L]\), the resulting Fourier series is a periodic function defined on \(\mathbb{R}\) with period \(L\).&lt;/p&gt;

&lt;p&gt;The above is only concerned about \(f(x)\in\mathbb{R}\), what happens if \(f(x)\in\mathbb{C}\)? We can use a different set of complex functions \(e^{ikx}\), which using Euler’s equation we have&lt;/p&gt;

\[e^{ikx} = \cos(kx) + i\sin(kx).\]

&lt;p&gt;We can easily prove that the \(e^{ikx}\)’s form an orthogonal basis&lt;/p&gt;

\[\begin{align*}
\Big\langle e^{ijx}, e^{ikx}\Big\rangle &amp;amp;= \int_{-\pi}^{\pi}{e^{ijx}(e^{ikx})^*dx}\\
&amp;amp;= \int_{-\pi}^{\pi}{e^{ijx}e^{-ikx}dx}\\
&amp;amp;= \int_{-\pi}^{\pi}{e^{i(j-k)x}dx}\\
&amp;amp;= \frac{1}{i(j-k)}e^{i(j-k)x}\Big|_{-\pi}^{\pi} = \begin{cases}
0 &amp;amp; j\neq k\\ 
2\pi &amp;amp; j = k.
\end{cases}
\end{align*}\]

&lt;p&gt;Then for \(f(x)\) defined on \([-\pi, \pi]\) we have:
\(f(x) = \sum_{k=-\infty}^{\infty}c_ke^{ikx} = \sum_{k=-\infty}^{\infty}(\alpha_k + i\beta_k)\Big[\cos(kx) + i\sin(kx)\Big],\)&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

\[c_k = \frac{1}{2\pi}\Big\langle f(x), e^{ikx}\Big\rangle.\]

&lt;h1 id=&quot;fourier-transform&quot;&gt;Fourier Transform&lt;/h1&gt;

&lt;p&gt;Till now, we only dealt with signals that are periodic, for non-periodic signals we will have to use a new tool: Fourier transform. Let’s first see what happens if we write out the complex Fourier series for signals \(f(x)\) that are only defined on \([-L, L]\), then we will get a \(f(x)\) using Fourier series that is \(2L\) periodic. If \(L\rightarrow\infty\), then \(f(x)\) becomes a non-periodic signal, and we will see what happens as it approaches this limit.&lt;/p&gt;

&lt;p&gt;As for \(f(x)\) that are defined on \([0, L]\), we need to first see what happens to the sine and cosine functions if they are \(2L\) or \(2L/k\) periodic&lt;/p&gt;

\[\frac{2\pi}{m} = \frac{2L}{k} \rightarrow m = \frac{k\pi}{L}.\]

&lt;p&gt;Thus we have&lt;/p&gt;

\[\cos(\frac{k\pi}{L}x) + i\sin(\frac{k\pi}{L}x) = e^{ik\pi x/L},\]

&lt;p&gt;the corresponding Fourier series becomes&lt;/p&gt;

\[f(x) = \sum_{k=-\infty}^{\infty}c_ke^{ik\pi x/L},\]

&lt;p&gt;with&lt;/p&gt;

\[c_k = \frac{1}{2\pi}\Big\langle f(x), e^{ik\pi x/L}\Big\rangle = \frac{1}{2L}\int_{-L}^{L}{f(x)e^{-ik\pi x/L}dx}.\]

&lt;p&gt;To simplify the notation we define \(\Delta{\omega} = \pi/L\), as \(L\rightarrow\infty\) we
have \(\Delta{\omega}\rightarrow0\). We then have&lt;/p&gt;

\[\begin{align*}
f(x) &amp;amp;= \lim_{L\rightarrow\infty}\sum_{k=-\infty}^{\infty}c_ke^{ik\pi x/L}\\
&amp;amp;=
\lim_{L\rightarrow\infty}\sum_{k=-\infty}^{\infty}\Bigg[\frac{1}{2L}\int_{-L}^{L}{f(x)e^{-ik\pi
x/L}dx}\Bigg]e^{ik\pi x/L}\\
&amp;amp;=
\lim_{L\rightarrow\infty}\sum_{k=-\infty}^{\infty}\Bigg[\frac{1}{2L}\int_{-L}^{L}{f(\xi)e^{-ik\pi
\xi/L}d\xi}\Bigg]e^{ik\pi x/L} &amp;amp; \mathrm{replace}\ x\ \mathrm{using\ a\ dummy\ variable\
}\xi\\
&amp;amp;=
\lim_{\Delta{\omega}\rightarrow0}\sum_{k=-\infty}^{\infty}\Bigg[\frac{\Delta{\omega}}{2\pi}\int_{-\pi/\Delta{\omega}}^{\pi/\Delta{\omega}}{f(\xi)e^{-ik\Delta{\omega}
\xi}d\xi}\Bigg]e^{ik\Delta{\omega}x} &amp;amp; \mathrm{replace}\ L\ \mathrm{with}\ \Delta{\omega}\\
&amp;amp;=
\lim_{\Delta{\omega}\rightarrow0}\sum_{k=-\infty}^{\infty}\Bigg[\frac{1}{2\pi}\int_{-\pi/\Delta{\omega}}^{\pi/\Delta{\omega}}{f(\xi)e^{-ik\Delta{\omega}
\xi}d\xi}\Bigg]e^{ik\Delta{\omega}x}\Delta{\omega}\\
&amp;amp;=
\int_{-\infty}^{\infty}\frac{1}{2\pi}\Bigg[\int_{-\infty}^{\infty}{f(\xi)e^{-i\omega\xi}d\xi}\Bigg]e^{i\omega
x}d\omega\\
&amp;amp;= \frac{1}{2\pi}\int_{-\infty}^{\infty}\widehat{f}(\omega)e^{i\omega x}d\omega.
\end{align*}\]

&lt;p&gt;This gives us a forward and an inverse Fourier transform, or a Fourier transform pair&lt;/p&gt;

\[\begin{align*}
\widehat{f}(\omega) &amp;amp;= \mathcal{F}\Big(f(x)\Big) = \int_{-\infty}^{\infty}{f(x)e^{-i\omega
x}dx}\\
f(x) &amp;amp;= \mathcal{F}^{-1}\Big(\widehat{f}(\omega)\Big) =
\frac{1}{2\pi}\int_{-\infty}^{\infty}\widehat{f}(\omega)e^{i\omega x}d\omega.
\end{align*}\]

&lt;h1 id=&quot;discrete-fourier-transform&quot;&gt;Discrete Fourier Transform&lt;/h1&gt;

&lt;p&gt;One thing to note is that using discrete Fourier transform (DFT) you are actually generating a periodic function, where it is the same on the finite interval where the original function \(f(x)\) is defined. So a more suitable name maybe discrete Fourier series. Another widely used algorithm is the fast Fourier transform (FFT). The relationship between FFT and DFT is: FFT is a way to numerically realize DFT.&lt;/p&gt;

&lt;p&gt;The forward and inverse pair of DFT is&lt;/p&gt;

\[\begin{align*}
\hat{f}_k &amp;amp;= \sum_{j=0}^{N-1}{f_je^{-i2\pi jk/n}}\\
f_k &amp;amp;= \frac{1}{n}\sum_{j=0}^{N-1}{\hat{f}_je^{i2\pi jk/n}}.
\end{align*}\]

&lt;p&gt;We can define \(\omega_n = e^{-i2\pi/n}\), then we can write the relationship between the
discrete sampled data points \(f_k\)’s and the Fourier series coefficient \(\hat{f}_k\)’s&lt;/p&gt;

\[\begin{bmatrix}
\hat{f}_0\\
\hat{f}_1\\
\hat{f}_2\\
\vdots\\
\hat{f}_n
\end{bmatrix} = \underbrace{\begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1\\
1 &amp;amp; \omega_n &amp;amp; \omega_n^2 &amp;amp; \cdots &amp;amp; \omega_n^{(n-1)}\\
1 &amp;amp; \omega_n^2 &amp;amp; \omega_n^4 &amp;amp; \cdots &amp;amp; \omega_n^{2(n-1)}\\
\vdots\ &amp;amp; \vdots\ &amp;amp; \vdots\ &amp;amp; \ddots &amp;amp; \vdots\\\
1 &amp;amp; \omega_n^{(n-1)} &amp;amp; \omega_n^{2(n-1)} &amp;amp; \cdots &amp;amp; \omega_n^{(n-1)^2}\\
\end{bmatrix}}_\mathrm{DFT\ Matrix}\begin{bmatrix}
f_0\\
f_1\\
f_2\\
\vdots\\
f_n
\end{bmatrix}\]

&lt;h1 id=&quot;applications&quot;&gt;Applications&lt;/h1&gt;

&lt;p&gt;This section we will talk about how we can apply Fourier transform to solve a variety of problems. These include a way to calculate derivative that has higher accuracy than simply doing finite differencing, transforming a convolution into multiplication in the frequency domain.&lt;/p&gt;

&lt;p&gt;We will show how taking the Fourier transform of derivatives gives a nice way to calculate derivatives
\(\begin{align*}
\mathcal{F}\Big(\frac{d}{dt}f(x)\Big) &amp;amp;=
\int_{-\infty}^{\infty}{\frac{d}{dt}f(x)e^{-i\omega x}dx}\\
&amp;amp;= \int_{-\infty}^{\infty}{\frac{d}{dt}f(x)e^{-iwx}dx} &amp;amp;
\text{perform integration by parts}\ \int{duv} = uv - \int{udv}\\
&amp;amp;= f(x)e^{-i\omega x}\Big|_{-\infty}^{\infty} +
i\omega\int_{-\infty}^{\infty}{f(x)e^{-i\omega x}dx}\\
&amp;amp;= 0 + i\omega\int_{-\infty}^{\infty}{f(x)e^{-i\omega x}dx} &amp;amp; \text{as}\
x\rightarrow\pm\infty,\ f(x)\rightarrow0\\
&amp;amp;= i\omega\widehat{f}(\omega) = i\omega\mathcal{F}\Big(f(x)\Big)
\end{align*}\)&lt;/p&gt;

&lt;p&gt;Now let’s look into the relationship between convolutions and multiplication in the Fourier domain. A convolution is defined as&lt;/p&gt;

\[f(x)*g(x). = \int_{-\infty}^{\infty}{f(x-\xi)g(\xi)d\xi}.\]

&lt;p&gt;If we take the inverse Fourier transform of the multiplication of \(\widehat{f}(\omega)\) and \(\widehat{g}(\omega)\) we have&lt;/p&gt;

\[\begin{align*}
\mathcal{F}^{-1}\Big(\widehat{f}(\omega)\widehat{g}(\omega)\Big) &amp;amp;=
\frac{1}{2\pi}\int_{-\infty}^{\infty}{\widehat{f}(\omega)\widehat{g}(\omega)e^{i\omega
x}d\omega}\\
&amp;amp;=
\frac{1}{2\pi}\int_{-\infty}^{\infty}{\widehat{f}(\omega)\Big[\int_{-\infty}^{\infty}{g(y)e^{-i\omega
y}dy}\Big]e^{i\omega
x}d\omega}\\
&amp;amp;=
\frac{1}{2\pi}\int_{-\infty}^{\infty}{\int_{-\infty}^{\infty}{\widehat{f}(\omega)g(y)e^{-i\omega
y}e^{i\omega x}dy}d\omega}\\
&amp;amp;=
\frac{1}{2\pi}\int_{-\infty}^{\infty}{\int_{-\infty}^{\infty}{\widehat{f}(\omega)g(y)e^{i\omega
(x-y)}d\omega}dy} &amp;amp; \text{commutative property of double integrals}\\
&amp;amp;=
\frac{1}{2\pi}\int_{-\infty}^{\infty}{g(y)\int_{-\infty}^{\infty}{\widehat{f}(\omega)e^{i\omega
(x-y)}d\omega}dy}\\
&amp;amp;= \int_{-\infty}^{\infty}{g(y)f(x - y)dy} &amp;amp; f(x - y) =
\mathcal{F}^{-1}(\widehat{f}(\omega)) =
\frac{1}{2\pi}\int_{-\infty}^{\infty}\widehat{f}(\omega)e^{i\omega(x-y)}d\omega\\
&amp;amp;= f(x)*g(x).
\end{align*}\]

&lt;p&gt;Thus we have&lt;/p&gt;

\[\mathcal{F}(f(x)*g(x)) = \mathcal{F}(f(x)\mathcal{F}(g(x)) = \widehat{f}(\omega)\widehat{g}(\omega).\]

&lt;h1 id=&quot;laplace-transform&quot;&gt;Laplace Transform&lt;/h1&gt;

&lt;p&gt;One limitation for Fourier transform is that it is only able to deal with functions that decay to zero as it approaches both positive and negative infinity. To deal with functions that does not decay to zero you would need to utilize the Laplace transform. Essentially, the Laplace transform constructs a new well-behaved function \(F(x)\) from the original function \(f(x)\),&lt;/p&gt;

\[F(x) = f(x)e^{-\gamma x}H(x) = \begin{cases}
f(x)e^{-\gamma x} &amp;amp; x\geq0\\
0 &amp;amp; x\leq0
\end{cases},\]

&lt;p&gt;with \(H(x)\) being the heaviside function&lt;/p&gt;

\[H(x) = \begin{cases}
1 &amp;amp; x\geq0\\
0 &amp;amp; x\leq0
\end{cases}.\]

&lt;p&gt;We can see that \(F(x)\) now is a well-behaved function that decays to zero as \(x\) approaches both positive and negative infinity. Then we can simply perform the Fourier transform on \(F(x)\)&lt;/p&gt;

\[\begin{align*}
\hat{F}(x) &amp;amp;= \int_{-\infty}^{\infty}{F(x)e^{-i\omega x}dx}\\
&amp;amp;= \int_{-\infty}^{\infty}{f(x)e^{-\gamma x}H(x)e^{-i\omega x}dx} &amp;amp; F(x) = f(x)e^{-\gamma
x}H(x)\\
&amp;amp;= \int_{0}^{\infty}{f(x)e^{-\gamma x}e^{-i\omega x}dx} &amp;amp; H(x) = 0\ \mathrm{for}\ x \leq 0\\
&amp;amp;= \int_{0}^{\infty}{f(x)e^{-(\gamma + i\omega) x}dx}\\
&amp;amp;= \int_{0}^{\infty}{f(x)e^{-sx}dx} &amp;amp; s = \gamma + i\omega\\
&amp;amp;= \bar{f}(s),
\end{align*}\]

&lt;p&gt;this is the forward Laplace transform. For the inverse Laplace transform, we can simply perform the inverse Fourier transform on \(\hat{F}(x)\)&lt;/p&gt;

\[\begin{align*}
f(x) &amp;amp;= e^{\gamma x}F(x) \\
&amp;amp;= e^{\gamma x}\frac{1}{2\pi}\int_{-\infty}^{\infty}{\hat{F}(x)e^{i\omega
x}d\omega}\\
&amp;amp;= \frac{1}{2\pi}\int_{-\infty}^{\infty}{\bar{f}(s)e^{\gamma + i\omega x}d\omega}\\
&amp;amp;= \frac{1}{2\pi}\int_{-\infty}^{\infty}{\bar{f}(s)e^{sx}d\omega}\\
&amp;amp;= \frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty}{\bar{f}(s)e^{sx}ds} &amp;amp; ds =
id\omega.
\end{align*}\]

&lt;p&gt;And we have the Laplace transform pair&lt;/p&gt;

\[\begin{align*}
\bar{f}(s) &amp;amp;= \int_{0}^{\infty}{f(x)e^{-sx}dx}\\
f(x) &amp;amp;= \frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty}{\bar{f}(s)e^{sx}ds}.
\end{align*}\]

&lt;p&gt;Note that the recovered \(f(x)\) only consists of the positive \(x\) portion, however, for most control related cases that would be enough due to nothing happening when time is negative.&lt;/p&gt;

&lt;p&gt;Some properties of the Laplace transform:&lt;/p&gt;

\[\begin{align}
\mathcal{L}\{f(x)\} &amp;amp;= \bar{f}(s)\\
\mathcal{L}\Big\{\frac{df}{dx}\Big\} &amp;amp;= -f(0) + s\bar{f}(s)\\
\mathcal{L}\Big\{f(x)*g(x)\Big\} &amp;amp;= \mathcal{L}\{f(x)\}\mathcal{L}\{g(x)\} =
\bar{f}(s)\bar{g}(s).
\end{align}\]

&lt;d-byline&gt;&lt;/d-byline&gt;

&lt;p class=&quot;citation&quot;&gt;
    Credit to Steve Brunton, powered by &lt;a href=&quot;https://www.mathjax.org&quot;&gt;
    &lt;img title=&quot;Powered by MathJax&quot; src=&quot;https://www.mathjax.org/badge/mj_logo.png&quot; style=&quot;border:0;&quot; alt=&quot;Powered by MathJax&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html">The goal of this blog post is to discuss breifly what is Fourier transform and Fourier series, show the intuition behind them and talk about how to obtain them in real life.</summary></entry><entry><title type="html">Important Part of Gradient Descent Convergence Proof</title><link href="http://localhost:4000/2021/02/10/part_of_grad_desc_proof.html" rel="alternate" type="text/html" title="Important Part of Gradient Descent Convergence Proof" /><published>2021-02-10T00:00:00-05:00</published><updated>2021-02-10T00:00:00-05:00</updated><id>http://localhost:4000/2021/02/10/part_of_grad_desc_proof</id><content type="html" xml:base="http://localhost:4000/2021/02/10/part_of_grad_desc_proof.html">&lt;p&gt;This blog post proves a crucial statement in the gradient descent convergence analysis: from the assumption \(f\) is convex, twice differentiable function and \(\nabla{f}\) is Lipschitz continuous with constant \(L&amp;gt;0\) we have&lt;/p&gt;

\[f(y) \leq f(x) + \nabla{f}(x)^T(y - x) + \frac{L}{2}\|y-x\|_2^2.\]

&lt;p&gt;Here we list four statements:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) \(\nabla{f}\) is Lipschitz continuous with constant \(L&amp;gt;0\)&lt;/li&gt;
  &lt;li&gt;(2) \((\nabla{f}(x)-\nabla{f}(y))^T(x-y)\leq L\|x-y\|_2^2\)&lt;/li&gt;
  &lt;li&gt;(3) \(\nabla^2{f}(x)\preceq LI\)&lt;/li&gt;
  &lt;li&gt;(4) \(f(y) \leq f(x) + \nabla{f}(x)^T(y - x) + \frac{L}{2}\|y-x\|_2^2\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The outline of the proof is \(\mathrm{i}\rightarrow \mathrm{iii} \rightarrow \mathrm{iv}\) but the proof of \(\mathrm{i}\rightarrow \mathrm{ii}\) is also given.&lt;/p&gt;

&lt;h2 id=&quot;i--ii&quot;&gt;i → ii&lt;/h2&gt;

&lt;p&gt;From \(\nabla{f}\) is Lipschitz continuous with constant \(L&amp;gt;0\) we know \(\|\nabla{f}(x) - \nabla{f}(y)\|_2\leq L\|x-y\|_2\). From the Cauchy-Schwartz inequality we have&lt;/p&gt;

\[|\langle u, v\rangle| \leq \|u\|_2\|v\|_2.\]

&lt;p&gt;Setting \(u = \nabla{f}(x) - \nabla{f}(y)\) and \(v = x - y\) we have&lt;/p&gt;

\[\begin{align*}
|(\nabla{f}(x) - \nabla{f}(y))^T(x - y)| &amp;amp;\leq \|\nabla{f}(x) - \nabla{f}(y)\|_2\|x - y\|_2\\
&amp;amp;\leq L\|x-y\|_2\|x - y\|_2 &amp;amp; \mathrm{Lipschitz\ condition}\\
&amp;amp;\leq L\|x-y\|_2^2.
\end{align*}\]

&lt;p&gt;Since we have the absolute value of \((\nabla{f}(x) - \nabla{f}(y))^T(x - y)\) less than or equal to
\(L\|x-y\|_2^2\), we definetely have&lt;/p&gt;

\[(\nabla{f}(x)-\nabla{f}(x))^T(x-y)\leq L\|x-y\|_2^2.\]

&lt;h2 id=&quot;i--iii&quot;&gt;i → iii&lt;/h2&gt;

&lt;p&gt;From the mean value theorem we can have&lt;/p&gt;

&lt;p&gt;\(\frac{\nabla f(x + \Delta{x}) - \nabla f(x)}{\Delta{x}} = \nabla^2f(x + t\Delta{x}),\ t\in[0, 1].\)
If we multiply both sides with \(\Delta{x}\) and take the norm on both sides gives us&lt;/p&gt;

\[\begin{align*}
\|\nabla^2f(x + t\Delta{x})\Delta{x}\| &amp;amp;= \|\nabla f(x + \Delta{x}) - \nabla f(x)\|\\
\|\nabla^2f(x + t\Delta{x})\|\|\Delta{x}\| &amp;amp;\leq L\|\Delta{x}\|\\
\|\nabla^2f(x + t\Delta{x})\| &amp;amp;\leq L\\
\|\nabla^2f(x)\| &amp;amp;\leq L &amp;amp; \mathrm{set}\ t\rightarrow 0\
\end{align*}\]

&lt;p&gt;Since \(f\) is a convex function, we know its Hessian is positive semidefinite. Thus,
\(\|\nabla^2f(x)\|\) represents the spectral norm, which is equal to its largest eigenvalue. Thus we
have \(\nabla^2{f}(x)\preceq LI\). See definition 7 and fact 8 of &lt;a href=&quot;/assets/images/convex_opt/NotesMatrices.pdf&quot;&gt;this note&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;iii--iv&quot;&gt;iii → iv&lt;/h3&gt;

&lt;p&gt;From the second-order Taylor series we have&lt;/p&gt;

\[f(y) = f(x) + \nabla f(x)^T(y - x) + \frac{1}{2}(y - x)^T\nabla^2{f}(x)(y - x).\]

&lt;p&gt;From \(\nabla^2{f}(x)\preceq LI\) we have&lt;/p&gt;

\[\frac{1}{2}(y - x)^T\nabla^2{f}(x)(y - x) \leq \frac{1}{2}(y - x)^TLI(y - x) = \frac{L}{2}\|y-x\|_2^2.\]

&lt;p&gt;Thus we have&lt;/p&gt;

\[f(y) \leq f(x) + \nabla{f}(x)^T(y - x) + \frac{L}{2}\|y-x\|_2^2.\]

&lt;h2 id=&quot;i--iv&quot;&gt;i → iv&lt;/h2&gt;

&lt;p&gt;Prof.Joan Bruna’s mentioned in his class a way to prove iv from i which I did not think of before, thus I will include it here. If we have \(F(t) = f\big(y + t(x-y)\big)\), then using the fundamental theorem of calculus&lt;/p&gt;

\[F(b) - F(a) = \int_{b}^{a}{f(t)dt},\ \mathrm{if\ }F^\prime(t) = f(t),\]

&lt;p&gt;we can have&lt;/p&gt;

\[\begin{align}
F(b) - F(a) &amp;amp;= \int_{b}^{a}{F^\prime(t)dt}\\
&amp;amp;= \int_{b}^{a}{(x-y)\nabla{f}\big(y + t(x-y)\big)dt}.
\end{align}\]

&lt;p&gt;By setting \(a = 1\) and \(b = 0\) we have&lt;/p&gt;

\[F(b) - F(a) = f(x) - f(y) = \int_{0}^{1}{(x-y)\nabla{f}\big(y + t(x-y)\big)dt},\]

&lt;p&gt;which can be written as&lt;/p&gt;

\[f(x) - f(y) = \int_{0}^{1}{\Big\langle\nabla{f}\big(y + t(x-y)\big), (x-y)\Big\rangle dt}.\]

&lt;p&gt;Also we need to note that for a constant \(s\) or function \(s(x)\) with no \(t\) in the input, we have \(F(x) = s(x)t\) and \(dF(x)/dt = s(x)\). Thus, using the fundamental theorem of calculus we have&lt;/p&gt;

\[s(x) = \int_{0}^{1}s(x)dt = s(x)t\Big|_0^1 = s(x) - 0.\]

&lt;p&gt;Therefore, we have&lt;/p&gt;

\[\begin{align}
\Bigg|f(x) - f(y) - \langle\nabla f(y), x-y\rangle\Bigg| &amp;amp;=
\Bigg|\int_{0}^{1}{\Big\langle\nabla{f}\big(y +
t(x-y)\big), x-y\Big\rangle dt} - \int_{0}^{1}{\langle\nabla f(y), x-y\rangle dt}\Bigg|\\
&amp;amp;= \Bigg|\int_{0}^{1}{\Big\langle\Big[\nabla{f}\big(y + t(x-y)\big) - \nabla f(y)\Big],
x-y\Big\rangle dt}\Bigg|\\
&amp;amp;\leq \int_{0}^{1}{\Bigg|\Big\langle\Big[\nabla{f}\big(y + t(x-y)\big) - \nabla f(y)\Big],
x-y\Big\rangle\Bigg|dt}\\
&amp;amp;\leq \int_{0}^{1}{\Big\|\nabla{f}\big(y + t(x-y)\big) - \nabla f(y)\Big\|\Big\|x-y\Big\|dt}\\
&amp;amp;\leq \int_{0}^{1}{L\Big\|t(x-y)\Big\|\Big\|x-y\Big\|dt}\\
&amp;amp;= L\|x-y\|^2\int_{0}^{1}{tdt}\\
&amp;amp;= \frac{L}{2}\|x-y\|^2.
\end{align}\]

&lt;p&gt;Then we finally have&lt;/p&gt;

\[f(x) \leq f(y) + \langle\nabla f(y), x-y\rangle + \frac{L}{2}\|x-y\|^2,\ \forall x, y.\]

&lt;d-byline&gt;&lt;/d-byline&gt;

&lt;p class=&quot;citation&quot;&gt;
    Credit to Ryan Tibshirani, powered by &lt;a href=&quot;https://www.mathjax.org&quot;&gt;
    &lt;img title=&quot;Powered by MathJax&quot; src=&quot;https://www.mathjax.org/badge/mj_logo.png&quot; style=&quot;border:0;&quot; alt=&quot;Powered by MathJax&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html">This blog post proves a crucial statement in the gradient descent convergence analysis: from the assumption \(f\) is convex, twice differentiable function and \(\nabla{f}\) is Lipschitz continuous with constant \(L&amp;gt;0\) we have</summary></entry></feed>